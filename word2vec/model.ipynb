{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import regex\n",
    "RANDOM_SEED = 42\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_val_test(df, props=[.8, .1, .1]):\n",
    "    assert round(sum(props), 2) == 1 and len(props) >= 2\n",
    "    train_df, test_df, val_df = None, None, None\n",
    "\n",
    "    train_size = int(props[0] * len(df))\n",
    "    val_size =  train_size + int(props[1] * len(df))\n",
    "    test_size =val_size + int(props[2] * len(df)) \n",
    "    train_df = df.iloc[0:train_size]\n",
    "    val_df = df.iloc[train_size:val_size]\n",
    "    test_df = df.iloc[val_size:test_size]\n",
    "    \n",
    "    return train_df, val_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "def download_embeddings(fasttetxt):\n",
    "    # https://fasttext.cc/docs/en/english-vectors.html\n",
    "    if fasttetxt:\n",
    "      wv = api.load(\"fasttext-wiki-news-subwords-300\")\n",
    "    else:\n",
    "      \n",
    "      wv = api.load(\"word2vec-google-news-300\")\n",
    "      print(\"\\nLoading complete!\\n\" +\n",
    "            \"Vocabulary size: {}\".format(len(wv.vocab)))\n",
    "    return wv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        id                                               text author\n",
      "0  id26305  [this, process, however, afforded, me, no, mea...      0\n",
      "1  id17569  [it, never, once, occurred, to, me, that, the,...      1\n",
      "2  id11008  [in, his, left, hand, was, a, gold, snuff, box...      0\n",
      "3  id27763  [how, lovely, is, spring, as, we, looked, from...      2\n",
      "4  id12958  [finding, nothing, else, not, even, gold, the,...      1\n",
      "5  id22965  [a, youth, passed, in, solitude, my, best, yea...      2\n",
      "6  id09674  [the, astronomer, perhaps, at, this, point, to...      0\n",
      "7  id13515  [the, surcingle, hung, in, ribands, from, my, ...      0\n",
      "8  id19322  [i, knew, that, you, could, not, say, to, your...      0\n",
      "9  id00912  [i, confess, that, neither, the, structure, of...      2\n"
     ]
    }
   ],
   "source": [
    "# Opening and preprocessing input file\n",
    "import gensim.models\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "from src.preprocess import clean_text\n",
    "\n",
    "data = pd.read_csv('train.csv', quotechar='\"')\n",
    "data.sample(frac=1)\n",
    "\n",
    "\n",
    "# to convert authors into numbers\n",
    "author_to_number = {\n",
    "    'EAP': 0,\n",
    "    'HPL': 1,\n",
    "    'MWS': 2\n",
    "    \n",
    "}\n",
    "\n",
    "# lowercase, removing punctuation and tookenize sentences. Converting labels to int\n",
    "training_text = \"\"\n",
    "for i in range(len(data)):\n",
    "\n",
    "    data['text'][i] = nltk.word_tokenize(regex.sub(r'[^\\w\\s]', '',data['text'][i].lower()))\n",
    "    data['author'][i] = author_to_number[data['author'][i]]\n",
    "\n",
    "print(data[0:10])\n",
    "\n",
    "from src.dataset import *\n",
    "\n",
    "# Splitting dataset and generating vocab\n",
    "train_df, val_df, test_df = split_train_val_test(data)\n",
    "train_vocab, reversed_vocab = generate_vocab_map(train_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOWNLOAD = False\n",
    "# Use fastext or word2vec\n",
    "FASTTEXT = True\n",
    "WINDOW_SIZE = 5\n",
    "\n",
    "EMBEDDING_DIM = 300\n",
    "HIDDEN_DIM = 128\n",
    "NUM_LAYERS = 1\n",
    "BIDIRECTIONAL = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading or generating word2vec embeddings\n",
    "\n",
    "if DOWNLOAD:\n",
    "    model = download_embeddings(FASTTEXT)\n",
    "else:\n",
    "    if FASTTEXT:\n",
    "        model = gensim.models.FastText(sentences=train_df['text'], size=EMBEDDING_DIM, window=WINDOW_SIZE)\n",
    "    else:\n",
    "        model = gensim.models.Word2Vec(sentences=train_df['text'], size=EMBEDDING_DIM, window=WINDOW_SIZE)\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.dataset import HeadlineDataset\n",
    "from torch.utils.data import RandomSampler\n",
    "\n",
    "train_dataset = HeadlineDataset(train_vocab, train_df,model.wv, FASTTEXT)\n",
    "val_dataset = HeadlineDataset(train_vocab, val_df,model.wv, FASTTEXT)\n",
    "test_dataset = HeadlineDataset(train_vocab, test_df,model.wv, FASTTEXT)\n",
    "\n",
    "# Now that we're wrapping our dataframes in PyTorch datsets, we can make use of PyTorch Random Samplers.\n",
    "train_sampler = RandomSampler(train_dataset)\n",
    "val_sampler = RandomSampler(val_dataset)\n",
    "test_sampler = RandomSampler(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.0995, -0.3079, -0.0357,  ...,  0.2323, -0.2142,  0.0434],\n",
      "         [-0.1150, -0.1435,  0.0627,  ...,  0.0615, -0.0976,  0.0174],\n",
      "         [-0.2974, -0.4264,  0.0451,  ...,  0.6023, -0.1577,  0.3000],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[-0.4754,  0.2456,  0.2098,  ..., -0.4770, -0.0720, -0.3481],\n",
      "         [-0.2718, -0.1379,  0.1869,  ..., -0.1586, -0.0490, -0.1049],\n",
      "         [-0.1904, -0.0636,  0.2704,  ..., -0.1842, -0.0692, -0.0625],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[-0.2750,  0.2690, -0.0782,  ..., -0.3894,  0.1058, -0.2220],\n",
      "         [-0.3078,  0.0842, -0.2697,  ...,  0.1029,  0.3394,  0.3717],\n",
      "         [-0.1242, -0.0115,  0.3669,  ..., -0.7316, -0.0899, -0.5041],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0995, -0.3079, -0.0357,  ...,  0.2323, -0.2142,  0.0434],\n",
      "         [-0.0876, -0.1006,  0.0335,  ...,  0.0359, -0.0426,  0.0128],\n",
      "         [-0.2974, -0.4264,  0.0451,  ...,  0.6023, -0.1577,  0.3000],\n",
      "         ...,\n",
      "         [-0.1551, -0.0887,  0.0705,  ...,  0.0294, -0.0363,  0.0233],\n",
      "         [-0.2109, -0.2361,  0.1503,  ..., -0.0157, -0.1227, -0.0762],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.2244, -0.1575,  0.0429,  ..., -0.3040, -0.3236, -0.2210],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.0594, -0.1444, -0.0800,  ..., -0.1080, -0.4810,  0.0231],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[-0.2342, -0.3908,  0.2997,  ..., -0.0693, -0.2265, -0.0950],\n",
      "         [-0.3315, -0.3164,  0.1189,  ...,  0.1913, -0.1042,  0.0790],\n",
      "         [-0.3228, -0.0125,  0.0181,  ..., -0.2427,  0.1270, -0.1116],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]]) tensor([0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 2, 2, 2, 2, 0, 1])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from src.dataset import collate_fn\n",
    "BATCH_SIZE = 16\n",
    "train_iterator = DataLoader(train_dataset, batch_size=BATCH_SIZE, sampler=train_sampler, collate_fn=collate_fn)\n",
    "val_iterator = DataLoader(val_dataset, batch_size=BATCH_SIZE, sampler=val_sampler, collate_fn=collate_fn)\n",
    "test_iterator = DataLoader(test_dataset, batch_size=BATCH_SIZE, sampler=test_sampler, collate_fn=collate_fn)\n",
    "\n",
    "for x, y in test_iterator:\n",
    "    print(x,y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models import ClassificationModel\n",
    "\n",
    "model = ClassificationModel(len(train_vocab),embedding_dim=EMBEDDING_DIM,hidden_dim = HIDDEN_DIM,num_layers = NUM_LAYERS,bidirectional = BIDIRECTIONAL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, **instantiate the model with some hyperparameters, and select an appropriate loss function and optimizer.** \n",
    "\n",
    "Hint: we already use sigmoid in our model. What loss functions are availible for binary classification? Feel free to look at PyTorch docs for help!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "criterion, optimizer = torch.nn.CrossEntropyLoss(), torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Training and Evaluation [10 Points]\n",
    "The final part of this HW involves training the model, and evaluating it at each epoch. **Fill out the train and test loops below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns the total loss calculated from criterion\n",
    "def train_loop(model, criterion, iterator):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for x, y in tqdm(iterator):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        prediction = model(x)\n",
    "        prediction = torch.squeeze(prediction)\n",
    "        # y = y.round()\n",
    "        # y = y.long()\n",
    "\n",
    " \n",
    "        loss = criterion(prediction,y)\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return total_loss\n",
    "\n",
    "# returns:\n",
    "# - true: a Python boolean array of all the ground truth values \n",
    "#         taken from the dataset iterator\n",
    "# - pred: a Python boolean array of all model predictions. \n",
    "def val_loop(model, criterion, iterator):\n",
    "    true, pred = [], []\n",
    "    for x, y in tqdm(iterator):\n",
    "    \n",
    "        preds = model(x)\n",
    "        preds = torch.squeeze(preds)\n",
    "        for i_batch in range(len(y)):\n",
    "            true.append(y[i_batch])\n",
    "            pred.append(torch.argmax(preds[i_batch]))\n",
    "            \n",
    "    return true, pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 123/123 [00:04<00:00, 25.64it/s]\n"
     ]
    }
   ],
   "source": [
    "# To test your eval implementation, let's see how well the untrained model does on our dev dataset.\n",
    "# It should do pretty poorly.\n",
    "from src.eval_utils import binary_macro_f1, accuracy\n",
    "true, pred = val_loop(model, criterion, val_iterator)\n",
    "# print(binary_macro_f1(true, pred))\n",
    "# print(accuracy(true, pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actually training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 979/979 [02:06<00:00,  7.76it/s]\n",
      "100%|██████████| 123/123 [00:04<00:00, 26.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 0\n",
      "TRAIN LOSS: 1052.448987007141\n",
      "VAL F-1: 0.46251639714955256\n",
      "VAL ACC: 0.5084312723556464\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 979/979 [02:02<00:00,  7.99it/s]\n",
      "100%|██████████| 123/123 [00:04<00:00, 26.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 1\n",
      "TRAIN LOSS: 969.1071000099182\n",
      "VAL F-1: 0.5357935487622151\n",
      "VAL ACC: 0.5355135411343894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 979/979 [02:03<00:00,  7.90it/s]\n",
      "100%|██████████| 123/123 [00:04<00:00, 27.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 2\n",
      "TRAIN LOSS: 929.1932902336121\n",
      "VAL F-1: 0.5400190012145475\n",
      "VAL ACC: 0.5631067961165048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 979/979 [02:02<00:00,  7.96it/s]\n",
      "100%|██████████| 123/123 [00:04<00:00, 27.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 3\n",
      "TRAIN LOSS: 906.6018470823765\n",
      "VAL F-1: 0.532068091943842\n",
      "VAL ACC: 0.5549310168625448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 979/979 [02:03<00:00,  7.92it/s]\n",
      "100%|██████████| 123/123 [00:04<00:00, 27.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 4\n",
      "TRAIN LOSS: 890.2176051437855\n",
      "VAL F-1: 0.5873037118926043\n",
      "VAL ACC: 0.594276954522228\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 979/979 [01:58<00:00,  8.23it/s]\n",
      "100%|██████████| 123/123 [00:04<00:00, 27.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 5\n",
      "TRAIN LOSS: 874.4431339800358\n",
      "VAL F-1: 0.5966272679591111\n",
      "VAL ACC: 0.596320899335718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 979/979 [01:58<00:00,  8.25it/s]\n",
      "100%|██████████| 123/123 [00:04<00:00, 26.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 6\n",
      "TRAIN LOSS: 858.5327359139919\n",
      "VAL F-1: 0.549451274479904\n",
      "VAL ACC: 0.5784363822176801\n"
     ]
    }
   ],
   "source": [
    "TOTAL_EPOCHS = 7\n",
    "for epoch in range(TOTAL_EPOCHS):\n",
    "    train_loss = train_loop(model, criterion, train_iterator)\n",
    "    true, pred = val_loop(model, criterion, val_iterator)\n",
    "    print(f\"EPOCH: {epoch}\")\n",
    "    print(f\"TRAIN LOSS: {train_loss}\")\n",
    "    print(f\"VAL F-1: {binary_macro_f1(true, pred)}\")\n",
    "    print(f\"VAL ACC: {accuracy(true, pred)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at the models performance on the held-out test set, using the same val_loop we wrote earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 123/123 [00:04<00:00, 27.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST F-1: 0.5483759168290464\n",
      "TEST ACC: 0.573326520183955\n"
     ]
    }
   ],
   "source": [
    "true, pred = val_loop(model, criterion, test_iterator)\n",
    "print(f\"TEST F-1: {binary_macro_f1(true, pred)}\")\n",
    "print(f\"TEST ACC: {accuracy(true, pred)}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8abf625e542ff33194dd4502f291ce11b7bf8daed732e6d5f31b5a030b27ce17"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
