{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1. Loading and Preprocessing Data \n",
    "The following cell loads the OnionOrNot dataset, and tokenizes each data item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT MODIFY #\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "RANDOM_SEED = 42\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "# this is how we select a GPU if it's avalible on your computer.\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_val_test(df, props=[.8, .1, .1]):\n",
    "    assert round(sum(props), 2) == 1 and len(props) >= 2\n",
    "    train_df, test_df, val_df = None, None, None\n",
    "    \n",
    "    ## YOUR CODE STARTS HERE (~3-5 lines of code) ##\n",
    "    # hint: you can use df.iloc to slice into specific indexes or ranges.\n",
    "    train_size = int(props[0] * len(df))\n",
    "    val_size =  train_size + int(props[1] * len(df))\n",
    "    test_size =val_size + int(props[2] * len(df)) \n",
    "    train_df = df.iloc[0:train_size]\n",
    "    val_df = df.iloc[train_size:val_size]\n",
    "    test_df = df.iloc[val_size:test_size]\n",
    "    \n",
    "\n",
    "    ## YOUR CODE ENDS HERE ##\n",
    "    \n",
    "    return train_df, val_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.models\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "from src.preprocess import clean_text\n",
    "\n",
    "data = pd.read_csv('train.csv', quotechar='\"')\n",
    "data.sample(frac=1)\n",
    "\n",
    "\n",
    "# to convert authors into numbers\n",
    "author_to_number = {\n",
    "    'EAP': 0,\n",
    "    'HPL': 1,\n",
    "    'MWS': 2\n",
    "    \n",
    "}\n",
    "\n",
    "# lowercase and tookenize sentences and converting authors to int\n",
    "training_text = \"\"\n",
    "for i in range(len(data)):\n",
    "\n",
    "    data['text'][i] = nltk.word_tokenize(data['text'][i].lower())\n",
    "    data['author'][i] = author_to_number[data['author'][i]]\n",
    "\n",
    "\n",
    "# print(train_df[0:100])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.35372332, -0.21892434, -0.42669693, -0.8065087 , -0.35411844,\n",
       "       -0.7134083 , -0.81613445,  1.2479382 , -0.0961616 , -0.27998012,\n",
       "        0.20446391,  0.7864846 ,  0.1925291 ,  0.07181507, -0.8196775 ,\n",
       "        0.37739325,  0.43254086, -0.9081421 , -0.35610607, -0.9284587 ,\n",
       "       -0.11410665,  0.81164134, -0.467723  ,  0.12861249,  0.3100781 ,\n",
       "        1.2033657 , -0.23873782, -0.5529341 ,  1.0321962 ,  0.30944267,\n",
       "        1.431958  , -0.1352361 ,  0.27077287, -0.22875233, -0.1075282 ,\n",
       "        1.1086254 , -0.5252576 , -0.8342886 , -0.10383235,  0.2144221 ,\n",
       "        0.12922513,  1.0267302 ,  0.01532739, -0.84992987,  0.3042318 ,\n",
       "        1.8826039 ,  0.41099328, -0.25291282, -0.2030294 ,  0.18681571,\n",
       "        0.24375334,  0.60119206,  0.6150842 , -0.08565544, -0.27811393,\n",
       "        0.30948603,  0.33192647,  0.7676214 ,  0.3884883 , -0.46885395,\n",
       "        0.20916714,  0.4344627 , -0.01126379,  0.30733675,  0.11881754,\n",
       "        0.04564675, -1.0912914 ,  0.0648623 , -1.1331195 , -0.56842786,\n",
       "        0.55481887, -0.01135838,  0.16564126,  0.53031766, -1.1449629 ,\n",
       "        0.03964809, -1.0333444 , -0.9409381 , -0.6587416 ,  0.57616913,\n",
       "       -0.6954531 ,  0.04603415, -0.06554519,  0.22738259, -0.13698448,\n",
       "        0.05682563, -0.4242104 ,  0.8130361 , -0.42671198,  0.7209153 ,\n",
       "       -0.42845774,  0.78432256,  1.1037779 ,  0.00889821,  0.37923074,\n",
       "       -1.3150618 ,  0.98042643, -0.6563781 , -0.31654406,  1.2523165 ,\n",
       "        0.69243735, -0.4126982 , -0.22953255, -0.46005628,  0.7931333 ,\n",
       "        0.08750858, -0.8423395 ,  0.80332613, -0.68300074, -0.52374107,\n",
       "       -1.3810624 , -0.17941926,  0.18793774,  0.65940404, -0.23867396,\n",
       "       -1.1291792 , -0.21611094,  0.23165935,  0.09655102,  0.08052856,\n",
       "        0.24369696,  0.25854734,  0.36985984, -0.8308578 ,  0.21682075,\n",
       "        1.187979  ,  0.10796509,  0.6457648 ,  0.5606276 ,  0.20607129,\n",
       "        0.6812318 ,  0.15505464, -0.51721853,  1.428455  ,  0.20324495,\n",
       "       -0.29184395, -1.6479756 ,  0.07889257,  1.2012668 , -0.25527284,\n",
       "       -0.2576772 , -0.13357623,  0.2752177 , -0.08826896, -0.17843205,\n",
       "        0.06851604, -0.21913429, -0.4897663 , -0.13876759,  0.16929363,\n",
       "        0.16546313,  0.8116047 , -0.19370463, -0.585508  , -0.2977666 ,\n",
       "       -0.31578177,  1.4708494 , -0.3666749 , -0.23677169, -0.19799863,\n",
       "        0.5378337 , -0.45095298,  0.15060791,  0.5396869 , -0.06156861,\n",
       "       -0.4388855 ,  0.8572863 ,  0.23939319,  0.86620516,  0.29456472,\n",
       "        0.6798292 ,  1.2755449 ,  0.60342425,  0.01193356, -0.9242643 ,\n",
       "        0.51204944, -0.46628973, -0.43015242, -0.30053428, -0.00733683,\n",
       "        0.06617155, -1.6877202 , -0.22281334,  0.07812525, -0.5357026 ,\n",
       "        1.0356115 ,  0.0300643 ,  0.9056686 ,  0.03352191,  0.64122343,\n",
       "        0.4839208 ,  0.28337985, -0.06810208, -0.06019015, -0.04271021,\n",
       "       -0.34837103,  0.1070808 ,  0.55876607, -0.14526802, -0.2821745 ,\n",
       "       -0.5278192 , -0.6879126 , -0.64313656,  0.7973975 ,  0.03135862,\n",
       "        0.19142061,  0.13305816, -0.51387477,  0.2857826 ,  0.48838392,\n",
       "       -0.05986042,  0.17866491,  0.37787446, -0.62694746,  0.13946936,\n",
       "       -0.14253423, -0.29415876, -0.03122626, -0.8895998 ,  0.15360287,\n",
       "       -0.3688912 ,  0.43919533, -0.23937345,  0.23451243, -0.4991112 ,\n",
       "       -0.72753924,  0.19482318,  0.33568296,  0.09268217,  1.4063059 ,\n",
       "        0.61493814,  1.1151834 ,  0.16280659, -0.7683176 ,  1.0352001 ,\n",
       "       -0.49537268,  0.92964303, -0.36231554, -0.483083  , -0.32918808,\n",
       "        1.242965  , -0.18416122, -0.40877733,  0.07439269,  0.49631754,\n",
       "       -0.80565894,  1.1390423 , -0.02075253,  0.28070238, -0.07200897,\n",
       "        0.41182598,  1.3353391 ,  0.19534919,  0.8012875 , -1.0688682 ,\n",
       "        0.6519119 ,  0.2513036 ,  0.12933733,  1.1950943 , -0.9427036 ,\n",
       "        0.13684098, -0.16120525, -0.01766678,  0.20913781, -0.16993752,\n",
       "       -1.0133865 , -0.33934295, -0.5581579 , -1.2466291 ,  1.1393499 ,\n",
       "       -0.53819245, -0.46690774, -0.11131272, -0.10467042, -0.45824608,\n",
       "       -0.79001856, -0.94429415, -0.01426683, -1.0806156 ,  2.1453743 ,\n",
       "       -0.5113033 , -1.2542595 ,  0.13722633, -1.410878  ,  0.3623186 ,\n",
       "        0.6828035 ,  0.7135652 , -0.56489813, -0.5596193 ,  2.7654452 ,\n",
       "       -1.2238209 ,  0.32066077,  0.26844284, -0.13868113,  0.5834795 ,\n",
       "        1.0373536 , -0.0300713 ,  0.34975308, -0.93366164,  0.28156403],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = gensim.models.Word2Vec(sentences=data['text'], \n",
    "                 window=5, \n",
    "                 size=300,  \n",
    "                 min_count=1\n",
    "                 )\n",
    "\n",
    "model.wv.get_vector('i')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        id                                               text author\n",
      "0  id26305  [this, process, ,, however, ,, afforded, me, n...      0\n",
      "1  id17569  [it, never, once, occurred, to, me, that, the,...      1\n",
      "2  id11008  [in, his, left, hand, was, a, gold, snuff, box...      0\n",
      "3  id27763  [how, lovely, is, spring, as, we, looked, from...      2\n",
      "4  id12958  [finding, nothing, else, ,, not, even, gold, ,...      1\n",
      "5  id22965  [a, youth, passed, in, solitude, ,, my, best, ...      2\n",
      "6  id09674  [the, astronomer, ,, perhaps, ,, at, this, poi...      0\n",
      "7  id13515  [the, surcingle, hung, in, ribands, from, my, ...      0\n",
      "8  id19322  [i, knew, that, you, could, not, say, to, your...      0\n",
      "9  id00912  [i, confess, that, neither, the, structure, of...      2\n"
     ]
    }
   ],
   "source": [
    "from src.dataset import *\n",
    "embeddings = []\n",
    "# for i in range(len(data)):\n",
    "#     embeddings_sentence = []\n",
    "#     for word in data['text'][i]:\n",
    "#         embeddings_sentence.append(model.wv.get_vector(word))\n",
    "#     embeddings.append(embeddings_sentence)\n",
    "# data.insert(3,'emb',embeddings)\n",
    "\n",
    "train_df, val_df, test_df = split_train_val_test(data)\n",
    "train_vocab, reversed_vocab = generate_vocab_map(train_df)\n",
    "\n",
    "print(train_df[0:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's what the dataset looks like. You can index into specific rows with pandas, and try to guess some of these yourself :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've loaded this dataset, we need to split the data into train, validation, and test sets. We also need to create a vocab map for words in our Onion dataset, which will map tokens to numbers. This will be useful later, since torch models can only use tensors of sequences of numbers as inputs. **Go to src/dataset.py, and fill out split_train_val_test, generate_vocab_map**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch has custom Datset Classes that have very useful extentions. **Go to src/dataset.py, and fill out the HeadlineDataset class.** Refer to PyTorch documentation on Dataset Classes for help."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use PyTorch DataLoaders to batch our data for us. **Go to src/dataset.py, and fill out collate_fn.** Refer to PyTorch documentation on Dataloaders for help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.dataset import HeadlineDataset\n",
    "from torch.utils.data import RandomSampler\n",
    "#print(train_df)\n",
    "\n",
    "train_dataset = HeadlineDataset(train_vocab, train_df,model.wv)\n",
    "val_dataset = HeadlineDataset(train_vocab, val_df,model.wv)\n",
    "test_dataset = HeadlineDataset(train_vocab, test_df,model.wv)\n",
    "\n",
    "# Now that we're wrapping our dataframes in PyTorch datsets, we can make use of PyTorch Random Samplers.\n",
    "train_sampler = RandomSampler(train_dataset)\n",
    "val_sampler = RandomSampler(val_dataset)\n",
    "test_sampler = RandomSampler(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from src.dataset import collate_fn\n",
    "BATCH_SIZE = 16\n",
    "train_iterator = DataLoader(train_dataset, batch_size=BATCH_SIZE, sampler=train_sampler, collate_fn=collate_fn)\n",
    "val_iterator = DataLoader(val_dataset, batch_size=BATCH_SIZE, sampler=val_sampler, collate_fn=collate_fn)\n",
    "test_iterator = DataLoader(test_dataset, batch_size=BATCH_SIZE, sampler=test_sampler, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-1.0413e-02, -1.8419e-01, -4.6175e-01,  ..., -2.9385e-01,\n",
      "          -4.0682e-01, -4.6937e-01],\n",
      "         [ 8.5743e-02, -7.8340e-02, -1.0163e-01,  ...,  7.3698e-02,\n",
      "          -2.0163e-01,  2.9778e-02],\n",
      "         [ 3.2238e-01, -2.6410e-01, -7.1182e-01,  ..., -1.3495e-01,\n",
      "          -1.9901e-01, -2.3080e-01],\n",
      "         ...,\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[-3.5372e-01, -2.1892e-01, -4.2670e-01,  ...,  3.4975e-01,\n",
      "          -9.3366e-01,  2.8156e-01],\n",
      "         [-1.4319e-01, -1.8945e-01, -2.4934e-01,  ...,  2.5600e-01,\n",
      "          -1.0182e+00,  1.6828e-01],\n",
      "         [-1.4680e-01, -1.0427e-02, -2.5578e-01,  ...,  1.6077e-01,\n",
      "          -1.0524e+00, -5.0740e-02],\n",
      "         ...,\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[-7.0630e-01, -2.8371e-01, -4.8866e-01,  ...,  1.5628e-01,\n",
      "          -1.1261e+00,  2.4533e-01],\n",
      "         [-1.1711e+00,  7.8100e-01, -3.7675e-01,  ..., -2.0416e-01,\n",
      "          -2.1694e+00, -1.7706e-01],\n",
      "         [-8.0769e-02, -5.3234e-01, -3.7576e-01,  ...,  1.4351e+00,\n",
      "          -1.3172e+00,  5.4211e-01],\n",
      "         ...,\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-1.0413e-02, -1.8419e-01, -4.6175e-01,  ..., -2.9385e-01,\n",
      "          -4.0682e-01, -4.6937e-01],\n",
      "         [ 4.3640e-02, -3.7748e-02, -9.0361e-02,  ...,  2.0953e-02,\n",
      "          -1.2898e-01,  6.7771e-03],\n",
      "         [ 3.2238e-01, -2.6410e-01, -7.1182e-01,  ..., -1.3495e-01,\n",
      "          -1.9901e-01, -2.3080e-01],\n",
      "         ...,\n",
      "         [-4.4087e-02, -2.7858e-03, -3.3831e-01,  ...,  1.6404e-01,\n",
      "          -4.3083e-01, -3.0214e-02],\n",
      "         [-5.2722e-03, -1.9167e-01, -3.5647e-01,  ...,  3.2268e-01,\n",
      "          -7.2386e-01,  2.4530e-01],\n",
      "         [-1.8619e-02,  4.4898e-02,  4.3155e-02,  ..., -3.8142e-03,\n",
      "          -1.6035e-01, -8.9407e-02]],\n",
      "\n",
      "        [[-2.1803e-02,  7.8450e-02, -1.0029e+00,  ...,  1.1274e+00,\n",
      "          -7.6827e-01, -2.5043e-01],\n",
      "         [-2.7452e-03, -5.2680e-03, -1.6981e-02,  ...,  1.9143e-02,\n",
      "          -1.5104e-02,  4.8500e-03],\n",
      "         [ 6.1243e-02, -4.6416e-04, -8.6484e-01,  ..., -3.1335e-02,\n",
      "          -1.3363e+00, -3.2217e-01],\n",
      "         ...,\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 1.3049e-02, -3.0258e-01, -3.2925e-01,  ...,  2.7795e-01,\n",
      "          -7.7533e-01,  9.2100e-02],\n",
      "         [ 1.6337e-01, -2.3812e-01, -3.7773e-01,  ..., -7.4454e-02,\n",
      "          -5.3081e-01,  1.0604e-02],\n",
      "         [-2.4657e-01, -8.5133e-02, -3.2807e-01,  ...,  2.5536e-01,\n",
      "          -1.4896e+00,  2.3746e-01],\n",
      "         ...,\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00]]]) tensor([0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 2., 2., 2., 2., 0., 1.])\n"
     ]
    }
   ],
   "source": [
    "# # Use this to test your collate_fn implementation.\n",
    "\n",
    "# # You can look at the shapes of x and y or put print \n",
    "# # statements in collate_fn while running this snippet\n",
    "\n",
    "for x, y in test_iterator:\n",
    "    print(x,y)\n",
    "    break\n",
    "\n",
    "# embeddings = []\n",
    "# for i in range(len(train_iterator.dataset.df)):\n",
    "#     embeddings_sentence = []\n",
    "#     for word in train_iterator.dataset.df['text'][i]:\n",
    "#         embeddings_sentence.append(model.wv.get_vector(word))\n",
    "#     embeddings.append(embeddings_sentence)\n",
    "# data.insert(3,'emb',embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Modeling [10 pts]\n",
    "Let's move to modeling, now that we have dataset iterators that batch our data for us. **Go to src/model.py, and follow the instructions in the file to create a basic neural network. Then, create your model using the class, and define hyperparameters.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models import ClassificationModel\n",
    "model = None\n",
    "### YOUR CODE GOES HERE (1 line of code) ###\n",
    "model = ClassificationModel(len(train_vocab),embedding_dim=32,hidden_dim = 32,num_layers = 1,bidirectional = True)\n",
    "\n",
    "# model.to(device)\n",
    "# # \n",
    "### YOUR CODE ENDS HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, **instantiate the model with some hyperparameters, and select an appropriate loss function and optimizer.** \n",
    "\n",
    "Hint: we already use sigmoid in our model. What loss functions are availible for binary classification? Feel free to look at PyTorch docs for help!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "criterion, optimizer = None, None\n",
    "### YOUR CODE GOES HERE ###\n",
    "criterion, optimizer = torch.nn.CrossEntropyLoss(), torch.optim.SGD(model.parameters(), lr=1e-4, momentum=0.9)\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.9)\n",
    "\n",
    "### YOUR CODE ENDS HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Training and Evaluation [10 Points]\n",
    "The final part of this HW involves training the model, and evaluating it at each epoch. **Fill out the train and test loops below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns the total loss calculated from criterion\n",
    "def train_loop(model, criterion, iterator):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for x, y in tqdm(iterator):\n",
    "        optimizer.zero_grad()\n",
    "        # x = x.to(device)\n",
    "        # y = y.to(device)\n",
    "        ### YOUR CODE STARTS HERE (~6 lines of code) ###\n",
    "        prediction = model(x)\n",
    "        prediction = torch.squeeze(prediction,0)\n",
    "        y = y.round()\n",
    "        y = y.long()\n",
    "\n",
    " \n",
    "        loss = criterion(prediction,y)\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    # scheduler.step()\n",
    "        ### YOUR CODE ENDS HERE ###\n",
    "    return total_loss\n",
    "\n",
    "# returns:\n",
    "# - true: a Python boolean array of all the ground truth values \n",
    "#         taken from the dataset iterator\n",
    "# - pred: a Python boolean array of all model predictions. \n",
    "def val_loop(model, criterion, iterator):\n",
    "    true, pred = [], []\n",
    "    ### YOUR CODE STARTS HERE (~8 lines of code) ###\n",
    "    for x, y in tqdm(iterator):\n",
    "        # x = x.to(device)\n",
    "        # y = y.to(device)\n",
    "        # print(\"x\",x)\n",
    "        # print(\"y\",y)  \n",
    "    \n",
    "        preds = model(x)\n",
    "        preds = torch.flatten(preds)\n",
    "        for i_batch in range(len(y)):\n",
    "            true.append(y[i_batch])\n",
    "            pred.append(torch.argmax(preds[i_batch]))\n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "    ### YOUR CODE ENDS HERE ###\n",
    "    return true, pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need evaluation metrics that tell us how well our model is doing on the validation set at each epoch. **Complete the functions in src/eval.py.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 123/123 [00:03<00:00, 34.96it/s]\n"
     ]
    }
   ],
   "source": [
    "# To test your eval implementation, let's see how well the untrained model does on our dev dataset.\n",
    "# It should do pretty poorly.\n",
    "from src.eval_utils import binary_macro_f1, accuracy\n",
    "true, pred = val_loop(model, criterion, val_iterator)\n",
    "# print(binary_macro_f1(true, pred))\n",
    "# print(accuracy(true, pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4: Actually training the model [1 point]\n",
    "Watch your model train :D You should be able to achieve a validation F-1 score of at least .8 if everything went correctly. **Feel free to adjust the number of epochs to prevent overfitting or underfitting.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 979/979 [01:37<00:00,  9.99it/s]\n",
      "100%|██████████| 123/123 [00:03<00:00, 31.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 0\n",
      "TRAIN LOSS: 1077.0252841711044\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "float division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8352/1379305196.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"EPOCH: {epoch}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"TRAIN LOSS: {train_loss}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"VAL F-1: {binary_macro_f1(true, pred)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"VAL ACC: {accuracy(true, pred)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/uni/gatech/fall/lang/project/nlp-project/word2vec/src/eval_utils.py\u001b[0m in \u001b[0;36mbinary_macro_f1\u001b[0;34m(true, pred)\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mrecall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtp\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtp\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0mf1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mprecision\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mrecall\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprecision\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mrecall\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m     \u001b[0;31m## YOUR CODE ENDS HERE ##\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0mf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mf1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mf1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: float division by zero"
     ]
    }
   ],
   "source": [
    "TOTAL_EPOCHS = 7\n",
    "for epoch in range(TOTAL_EPOCHS):\n",
    "    train_loss = train_loop(model, criterion, train_iterator)\n",
    "    true, pred = val_loop(model, criterion, val_iterator)\n",
    "    print(f\"EPOCH: {epoch}\")\n",
    "    print(f\"TRAIN LOSS: {train_loss}\")\n",
    "    print(f\"VAL F-1: {binary_macro_f1(true, pred)}\")\n",
    "    print(f\"VAL ACC: {accuracy(true, pred)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at the models performance on the held-out test set, using the same val_loop we wrote earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 123/123 [00:03<00:00, 31.93it/s]\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_62669/3266028246.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"TEST F-1: {binary_macro_f1(true, pred)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"TEST ACC: {accuracy(true, pred)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/uni/gatech/fall/lang/project/nlp-project/word2vec/src/eval_utils.py\u001b[0m in \u001b[0;36mbinary_macro_f1\u001b[0;34m(true, pred)\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0maveraged_macro_f1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;31m## YOUR CODE STARTS HERE (1 line of code) ##\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m     \u001b[0maveraged_macro_f1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbinary_f1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mbinary_f1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m     \u001b[0;31m## YOUR CODE ENDS HERE ##\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0maveraged_macro_f1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/uni/gatech/fall/lang/project/nlp-project/word2vec/src/eval_utils.py\u001b[0m in \u001b[0;36mbinary_f1\u001b[0;34m(true, pred, selected_class)\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mtn\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mselected_class\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mprecision\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtp\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtp\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0mrecall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtp\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtp\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "true, pred = val_loop(model, criterion, test_iterator)\n",
    "print(f\"TEST F-1: {binary_macro_f1(true, pred)}\")\n",
    "print(f\"TEST ACC: {accuracy(true, pred)}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8abf625e542ff33194dd4502f291ce11b7bf8daed732e6d5f31b5a030b27ce17"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
