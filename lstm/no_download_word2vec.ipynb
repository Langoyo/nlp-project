{"nbformat":4,"nbformat_minor":5,"metadata":{"accelerator":"GPU","colab":{"name":"no_download/word2vec.ipynb","provenance":[],"collapsed_sections":[]},"interpreter":{"hash":"8abf625e542ff33194dd4502f291ce11b7bf8daed732e6d5f31b5a030b27ce17"},"kernelspec":{"display_name":"Python 3.9.5 64-bit ('base': conda)","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.5"}},"cells":[{"cell_type":"code","metadata":{"id":"1yv5wo80zFFD","executionInfo":{"status":"ok","timestamp":1638769247909,"user_tz":300,"elapsed":457,"user":{"displayName":"Andrés Langoyo Martín","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12914381003507187562"}}},"source":["import torch\n","import random\n","import numpy as np\n","import regex\n","\n","RANDOM_SEED = 42\n","torch.manual_seed(RANDOM_SEED)\n","random.seed(RANDOM_SEED)\n","np.random.seed(RANDOM_SEED)\n","\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"],"id":"1yv5wo80zFFD","execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"_4lpUGTQzFFI","executionInfo":{"status":"ok","timestamp":1638769248152,"user_tz":300,"elapsed":6,"user":{"displayName":"Andrés Langoyo Martín","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12914381003507187562"}}},"source":["def split_train_val_test(df, props=[.8, .1, .1]):\n","    assert round(sum(props), 2) == 1 and len(props) >= 2\n","    train_df, test_df, val_df = None, None, None\n","\n","    train_size = int(props[0] * len(df))\n","    val_size =  train_size + int(props[1] * len(df))\n","    test_size =val_size + int(props[2] * len(df)) \n","    train_df = df.iloc[0:train_size]\n","    val_df = df.iloc[train_size:val_size]\n","    test_df = df.iloc[val_size:test_size]\n","    \n","    return train_df, val_df, test_df"],"id":"_4lpUGTQzFFI","execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"u04VW4ugzFFI","executionInfo":{"status":"ok","timestamp":1638769248152,"user_tz":300,"elapsed":5,"user":{"displayName":"Andrés Langoyo Martín","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12914381003507187562"}}},"source":["import gensim.downloader as api\n","\n","def download_embeddings(fasttetxt):\n","    # https://fasttext.cc/docs/en/english-vectors.html\n","    if fasttetxt:\n","      wv = api.load(\"fasttext-wiki-news-subwords-300\")\n","    else:\n","      \n","      wv = api.load(\"word2vec-google-news-300\")\n","      print(\"\\nLoading complete!\\n\" +\n","            \"Vocabulary size: {}\".format(len(wv.vocab)))\n","    return wv\n"],"id":"u04VW4ugzFFI","execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"23WpDX0rzFFJ","executionInfo":{"status":"ok","timestamp":1638769261002,"user_tz":300,"elapsed":12854,"user":{"displayName":"Andrés Langoyo Martín","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12914381003507187562"}},"outputId":"c801cf11-2690-489f-d4f9-1c92075d74c7"},"source":["# Opening and preprocessing input file\n","import gensim.models\n","import pandas as pd\n","import nltk\n","nltk.download('punkt')\n","from tqdm import tqdm\n","from preprocess import clean_text\n","\n","data = pd.read_csv('train.csv', quotechar='\"')\n","data.sample(frac=1)\n","\n","\n","# to convert authors into numbers\n","author_to_number = {\n","    'EAP': 0,\n","    'HPL': 1,\n","    'MWS': 2\n","    \n","}\n","\n","# lowercase, removing punctuation and tookenize sentences. Converting labels to int\n","training_text = \"\"\n","for i in range(len(data)):\n","\n","    data['text'][i] = nltk.word_tokenize(regex.sub(r'[^\\w\\s]', '',data['text'][i].lower()))\n","    data['author'][i] = author_to_number[data['author'][i]]\n","\n","print(data[0:10])\n","print(len(data))\n","\n","from dataset import *\n","\n","# Splitting dataset and generating vocab\n","train_df, val_df, test_df = split_train_val_test(data)\n","train_vocab, reversed_vocab = generate_vocab_map(train_df)\n","\n"],"id":"23WpDX0rzFFJ","execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","        id                                               text author\n","0  id26305  [this, process, however, afforded, me, no, mea...      0\n","1  id17569  [it, never, once, occurred, to, me, that, the,...      1\n","2  id11008  [in, his, left, hand, was, a, gold, snuff, box...      0\n","3  id27763  [how, lovely, is, spring, as, we, looked, from...      2\n","4  id12958  [finding, nothing, else, not, even, gold, the,...      1\n","5  id22965  [a, youth, passed, in, solitude, my, best, yea...      2\n","6  id09674  [the, astronomer, perhaps, at, this, point, to...      0\n","7  id13515  [the, surcingle, hung, in, ribands, from, my, ...      0\n","8  id19322  [i, knew, that, you, could, not, say, to, your...      0\n","9  id00912  [i, confess, that, neither, the, structure, of...      2\n","19579\n"]}]},{"cell_type":"code","metadata":{"id":"NTDiU36-zFFK","executionInfo":{"status":"ok","timestamp":1638769261005,"user_tz":300,"elapsed":18,"user":{"displayName":"Andrés Langoyo Martín","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12914381003507187562"}}},"source":["DOWNLOAD = False\n","# Use fastext or word2vec\n","FASTTEXT = False\n","WINDOW_SIZE = 5\n","\n","EMBEDDING_DIM = 300\n","HIDDEN_DIM = 128\n","NUM_LAYERS = 1\n","BIDIRECTIONAL = True\n"],"id":"NTDiU36-zFFK","execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"kaqaR8CNzFFK","executionInfo":{"status":"ok","timestamp":1638769266665,"user_tz":300,"elapsed":5674,"user":{"displayName":"Andrés Langoyo Martín","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12914381003507187562"}}},"source":["# Downloading or generating word2vec embeddings\n","\n","if DOWNLOAD:\n","    model = download_embeddings(FASTTEXT)\n","else:\n","    if FASTTEXT:\n","        model = gensim.models.FastText(sentences=train_df['text'], size=EMBEDDING_DIM, window=WINDOW_SIZE)\n","    else:\n","        model = gensim.models.Word2Vec(sentences=train_df['text'], size=EMBEDDING_DIM, window=WINDOW_SIZE)\n","                        "],"id":"kaqaR8CNzFFK","execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"fVz5UQwTzFFL","executionInfo":{"status":"ok","timestamp":1638769266666,"user_tz":300,"elapsed":15,"user":{"displayName":"Andrés Langoyo Martín","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12914381003507187562"}}},"source":["from dataset import HeadlineDataset\n","from torch.utils.data import RandomSampler\n","\n","train_dataset = HeadlineDataset(train_vocab, train_df,model.wv, FASTTEXT)\n","val_dataset = HeadlineDataset(train_vocab, val_df,model.wv, FASTTEXT)\n","test_dataset = HeadlineDataset(train_vocab, test_df,model.wv, FASTTEXT)\n","\n","# Now that we're wrapping our dataframes in PyTorch datsets, we can make use of PyTorch Random Samplers.\n","train_sampler = RandomSampler(train_dataset)\n","val_sampler = RandomSampler(val_dataset)\n","test_sampler = RandomSampler(test_dataset)"],"id":"fVz5UQwTzFFL","execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gnfaq2kpzFFM","executionInfo":{"status":"ok","timestamp":1638769266885,"user_tz":300,"elapsed":230,"user":{"displayName":"Andrés Langoyo Martín","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12914381003507187562"}},"outputId":"bf5a0db7-01b4-4e23-f7a4-7305fdbee3c6"},"source":["from torch.utils.data import DataLoader\n","from dataset import collate_fn\n","BATCH_SIZE = 16\n","train_iterator = DataLoader(train_dataset, batch_size=BATCH_SIZE, sampler=train_sampler, collate_fn=collate_fn)\n","val_iterator = DataLoader(val_dataset, batch_size=BATCH_SIZE, sampler=val_sampler, collate_fn=collate_fn)\n","test_iterator = DataLoader(test_dataset, batch_size=BATCH_SIZE, sampler=test_sampler, collate_fn=collate_fn)\n","\n","for x, y in test_iterator:\n","    print(x,y)\n","    break"],"id":"gnfaq2kpzFFM","execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[[ 0.2023,  0.3327, -0.2675,  ..., -0.2955, -0.1995, -0.6266],\n","         [ 0.0498, -0.1170, -0.1079,  ..., -0.0727, -0.0963, -0.2080],\n","         [ 0.1343, -0.3369, -0.3732,  ..., -0.2670, -0.5220, -0.7150],\n","         ...,\n","         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n","\n","        [[-0.1035, -0.9070, -0.1016,  ..., -0.4207,  0.2501, -0.0609],\n","         [ 0.0454, -0.2868, -0.1376,  ..., -0.0651,  0.0015, -0.1849],\n","         [ 0.0106, -0.2581, -0.0899,  ...,  0.1391, -0.0131, -0.1398],\n","         ...,\n","         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n","\n","        [[-0.2933, -0.6308,  0.0843,  ..., -0.3770,  0.3463,  0.1079],\n","         [-0.4181, -0.2825, -0.1756,  ...,  0.1754,  0.5397,  0.2066],\n","         [ 0.0085, -0.6120,  0.0267,  ..., -0.1083,  0.0880, -0.2469],\n","         ...,\n","         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n","\n","        ...,\n","\n","        [[ 0.2023,  0.3327, -0.2675,  ..., -0.2955, -0.1995, -0.6266],\n","         [ 0.0328, -0.0662, -0.0741,  ..., -0.0642, -0.0616, -0.1324],\n","         [ 0.1343, -0.3369, -0.3732,  ..., -0.2670, -0.5220, -0.7150],\n","         ...,\n","         [-0.0282, -0.2957, -0.1225,  ..., -0.1218, -0.0598, -0.1728],\n","         [ 0.0821, -0.3034, -0.1704,  ..., -0.1243, -0.1027, -0.2470],\n","         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n","\n","        [[ 0.0260, -0.3711, -0.1677,  ..., -0.2355,  0.0085, -0.4943],\n","         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","         [ 0.1662,  0.1204, -0.1283,  ...,  0.0497,  0.0948, -0.6507],\n","         ...,\n","         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n","\n","        [[ 0.2299, -0.1360, -0.1985,  ..., -0.1006, -0.1590, -0.3132],\n","         [ 0.0777, -0.2631, -0.2227,  ..., -0.1329, -0.2358, -0.3906],\n","         [ 0.0394, -0.2711, -0.0958,  ..., -0.2146,  0.1313, -0.0908],\n","         ...,\n","         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]]) tensor([0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 2, 2, 2, 2, 0, 1])\n"]},{"output_type":"stream","name":"stderr","text":["/content/dataset.py:161: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n","  tokenized_word_tensor = torch.Tensor(tmp)\n"]}]},{"cell_type":"markdown","metadata":{"id":"FvFz28NgzFFM"},"source":["### Modeling"],"id":"FvFz28NgzFFM"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E54k4vH-zFFO","executionInfo":{"status":"ok","timestamp":1638769275485,"user_tz":300,"elapsed":8607,"user":{"displayName":"Andrés Langoyo Martín","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12914381003507187562"}},"outputId":"4c2d6e3e-59c7-404c-e22c-817f4e3850b1"},"source":["from models import ClassificationModel\n","\n","model = ClassificationModel(len(train_vocab),embedding_dim=EMBEDDING_DIM,hidden_dim = HIDDEN_DIM,num_layers = NUM_LAYERS,bidirectional = BIDIRECTIONAL)\n","\n","model.to(device)"],"id":"E54k4vH-zFFO","execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["ClassificationModel(\n","  (LSTM): LSTM(300, 128, batch_first=True, bidirectional=True)\n","  (linear): Linear(in_features=256, out_features=3, bias=True)\n","  (softmax): Softmax(dim=1)\n",")"]},"metadata":{},"execution_count":13}]},{"cell_type":"markdown","metadata":{"id":"PeoanbgVzFFO"},"source":["In the following cell, **instantiate the model with some hyperparameters, and select an appropriate loss function and optimizer.** \n","\n","Hint: we already use sigmoid in our model. What loss functions are availible for binary classification? Feel free to look at PyTorch docs for help!"],"id":"PeoanbgVzFFO"},{"cell_type":"code","metadata":{"id":"4rcqsWzHzFFP","executionInfo":{"status":"ok","timestamp":1638769275487,"user_tz":300,"elapsed":20,"user":{"displayName":"Andrés Langoyo Martín","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12914381003507187562"}}},"source":["from torch.optim import AdamW\n","\n","criterion, optimizer = torch.nn.CrossEntropyLoss(), torch.optim.Adam(model.parameters(), lr=0.001)"],"id":"4rcqsWzHzFFP","execution_count":14,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kNFrnFAVzFFP"},"source":["### Part 3: Training and Evaluation [10 Points]\n","The final part of this HW involves training the model, and evaluating it at each epoch. **Fill out the train and test loops below.**"],"id":"kNFrnFAVzFFP"},{"cell_type":"code","metadata":{"id":"4WOY7B-azFFP","executionInfo":{"status":"ok","timestamp":1638769275488,"user_tz":300,"elapsed":18,"user":{"displayName":"Andrés Langoyo Martín","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12914381003507187562"}}},"source":["# returns the total loss calculated from criterion\n","def train_loop(model, criterion, iterator):\n","    model.train()\n","    total_loss = 0\n","    \n","    for x, y in tqdm(iterator):\n","        x = x.to(device)\n","        y = y.to(device)\n","        optimizer.zero_grad()\n","\n","        prediction = model(x)\n","        prediction = torch.squeeze(prediction)\n","        # y = y.round()\n","        y = y.long()\n","        \n","\n"," \n","        loss = criterion(prediction,y)\n","        total_loss += loss.item()\n","        loss.backward()\n","        optimizer.step()\n","\n","    return total_loss\n","\n","# returns:\n","# - true: a Python boolean array of all the ground truth values \n","#         taken from the dataset iterator\n","# - pred: a Python boolean array of all model predictions. \n","def val_loop(model, criterion, iterator):\n","    true, pred = [], []\n","    for x, y in tqdm(iterator):\n","        x = x.to(device)\n","        y = y.to(device)\n","    \n","        preds = model(x)\n","        preds.to(device)\n","        preds = torch.squeeze(preds)\n","        for i_batch in range(len(y)):\n","            true.append(y[i_batch])\n","            pred.append(torch.argmax(preds[i_batch]))\n","            \n","    return true, pred\n"],"id":"4WOY7B-azFFP","execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RwyLMPAwzFFQ","executionInfo":{"status":"ok","timestamp":1638769280095,"user_tz":300,"elapsed":4623,"user":{"displayName":"Andrés Langoyo Martín","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12914381003507187562"}},"outputId":"6ef1e19c-86b6-4aed-8b66-09aaf0dbaf09"},"source":["# To test your eval implementation, let's see how well the untrained model does on our dev dataset.\n","# It should do pretty poorly.\n","from sklearn.metrics import f1_score, accuracy_score\n","\n","from eval_utils import binary_macro_f1, accuracy\n","true, pred = val_loop(model, criterion, val_iterator)\n","true = [x.item() for x in true]\n","pred = [x.item() for x in pred]\n","print(f1_score(true, pred, average='weighted'))\n","print(accuracy_score(true, pred))\n"],"id":"RwyLMPAwzFFQ","execution_count":16,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 123/123 [00:04<00:00, 27.21it/s]\n"]},{"output_type":"stream","name":"stdout","text":["0.26611359256628586\n","0.32447623914154317\n"]}]},{"cell_type":"markdown","metadata":{"id":"klS-hqCezFFQ"},"source":["### Actually training the model"],"id":"klS-hqCezFFQ"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PS8EQy8SzFFQ","executionInfo":{"status":"ok","timestamp":1638770530013,"user_tz":300,"elapsed":1249934,"user":{"displayName":"Andrés Langoyo Martín","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12914381003507187562"}},"outputId":"febb3786-125a-47f7-d5f4-61c8e4a9c2b1"},"source":["TOTAL_EPOCHS = 10\n","for epoch in range(TOTAL_EPOCHS):\n","    train_loss = train_loop(model, criterion, train_iterator)\n","    true, pred = val_loop(model, criterion, val_iterator)\n","    true = [x.item() for x in true]\n","    pred = [x.item() for x in pred]\n","    print(f\"EPOCH: {epoch}\")\n","    print(f\"TRAIN LOSS: {train_loss}\")\n","    print(f\"VAL F-1: {f1_score(true, pred, average='weighted')}\")\n","    print(f\"VAL ACC: {accuracy_score(true, pred)}\")\n"],"id":"PS8EQy8SzFFQ","execution_count":17,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 979/979 [02:00<00:00,  8.10it/s]\n","100%|██████████| 123/123 [00:04<00:00, 27.34it/s]\n"]},{"output_type":"stream","name":"stdout","text":["EPOCH: 0\n","TRAIN LOSS: 985.7116901874542\n","VAL F-1: 0.5715518445600153\n","VAL ACC: 0.5738375063873276\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 979/979 [02:00<00:00,  8.15it/s]\n","100%|██████████| 123/123 [00:04<00:00, 27.91it/s]\n"]},{"output_type":"stream","name":"stdout","text":["EPOCH: 1\n","TRAIN LOSS: 884.940908908844\n","VAL F-1: 0.5861754109844795\n","VAL ACC: 0.5896780786918753\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 979/979 [01:59<00:00,  8.17it/s]\n","100%|██████████| 123/123 [00:04<00:00, 27.65it/s]\n"]},{"output_type":"stream","name":"stdout","text":["EPOCH: 2\n","TRAIN LOSS: 830.1045944690704\n","VAL F-1: 0.5703736118841086\n","VAL ACC: 0.5866121614716403\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 979/979 [02:01<00:00,  8.07it/s]\n","100%|██████████| 123/123 [00:04<00:00, 27.22it/s]\n"]},{"output_type":"stream","name":"stdout","text":["EPOCH: 3\n","TRAIN LOSS: 792.5731391608715\n","VAL F-1: 0.6242709381839484\n","VAL ACC: 0.6259580991313235\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 979/979 [02:00<00:00,  8.13it/s]\n","100%|██████████| 123/123 [00:04<00:00, 27.59it/s]\n"]},{"output_type":"stream","name":"stdout","text":["EPOCH: 4\n","TRAIN LOSS: 757.3648587465286\n","VAL F-1: 0.6512133712877807\n","VAL ACC: 0.6515074092999489\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 979/979 [02:00<00:00,  8.10it/s]\n","100%|██████████| 123/123 [00:04<00:00, 27.34it/s]\n"]},{"output_type":"stream","name":"stdout","text":["EPOCH: 5\n","TRAIN LOSS: 718.3122003078461\n","VAL F-1: 0.6483401974954357\n","VAL ACC: 0.6504854368932039\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 979/979 [02:02<00:00,  8.01it/s]\n","100%|██████████| 123/123 [00:04<00:00, 27.53it/s]\n"]},{"output_type":"stream","name":"stdout","text":["EPOCH: 6\n","TRAIN LOSS: 683.0146762430668\n","VAL F-1: 0.6688205481596543\n","VAL ACC: 0.6693919264179867\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 979/979 [02:00<00:00,  8.14it/s]\n","100%|██████████| 123/123 [00:04<00:00, 27.81it/s]\n"]},{"output_type":"stream","name":"stdout","text":["EPOCH: 7\n","TRAIN LOSS: 644.8398031592369\n","VAL F-1: 0.6672287455535938\n","VAL ACC: 0.6668369954011242\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 979/979 [01:59<00:00,  8.21it/s]\n","100%|██████████| 123/123 [00:04<00:00, 27.89it/s]\n"]},{"output_type":"stream","name":"stdout","text":["EPOCH: 8\n","TRAIN LOSS: 605.1051699966192\n","VAL F-1: 0.6704467302558326\n","VAL ACC: 0.6709248850281042\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 979/979 [01:59<00:00,  8.21it/s]\n","100%|██████████| 123/123 [00:04<00:00, 27.70it/s]"]},{"output_type":"stream","name":"stdout","text":["EPOCH: 9\n","TRAIN LOSS: 561.894759863615\n","VAL F-1: 0.6641035162791222\n","VAL ACC: 0.6663260091977516\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"markdown","metadata":{"id":"ECkuloBSzFFR"},"source":["We can also look at the models performance on the held-out test set, using the same val_loop we wrote earlier."],"id":"ECkuloBSzFFR"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E9S33DF4zFFR","executionInfo":{"status":"ok","timestamp":1638770534390,"user_tz":300,"elapsed":4390,"user":{"displayName":"Andrés Langoyo Martín","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12914381003507187562"}},"outputId":"2f94c651-edfa-479e-8a41-458e65e0404f"},"source":["true, pred = val_loop(model, criterion, test_iterator)\n","true = [x.item() for x in true]\n","pred = [x.item() for x in pred]\n","print(f\"TEST F-1: {f1_score(true, pred, average='weighted')}\")\n","print(f\"TEST ACC: {accuracy_score(true, pred)}\")"],"id":"E9S33DF4zFFR","execution_count":18,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 123/123 [00:04<00:00, 28.00it/s]\n"]},{"output_type":"stream","name":"stdout","text":["TEST F-1: 0.6521270583403529\n","TEST ACC: 0.6535513541134389\n"]}]}]}