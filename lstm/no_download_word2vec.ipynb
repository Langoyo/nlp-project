{"nbformat":4,"nbformat_minor":5,"metadata":{"accelerator":"GPU","colab":{"name":"no_download/word2vec.ipynb","provenance":[],"collapsed_sections":[]},"interpreter":{"hash":"8abf625e542ff33194dd4502f291ce11b7bf8daed732e6d5f31b5a030b27ce17"},"kernelspec":{"display_name":"Python 3.9.5 64-bit ('base': conda)","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.5"}},"cells":[{"cell_type":"code","metadata":{"id":"1yv5wo80zFFD","executionInfo":{"status":"ok","timestamp":1638628875896,"user_tz":300,"elapsed":95,"user":{"displayName":"Andrés Langoyo Martín","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12914381003507187562"}}},"source":["import torch\n","import random\n","import numpy as np\n","import regex\n","\n","RANDOM_SEED = 42\n","torch.manual_seed(RANDOM_SEED)\n","random.seed(RANDOM_SEED)\n","np.random.seed(RANDOM_SEED)\n","\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"],"id":"1yv5wo80zFFD","execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"_4lpUGTQzFFI","executionInfo":{"status":"ok","timestamp":1638628875992,"user_tz":300,"elapsed":3,"user":{"displayName":"Andrés Langoyo Martín","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12914381003507187562"}}},"source":["def split_train_val_test(df, props=[.8, .1, .1]):\n","    assert round(sum(props), 2) == 1 and len(props) >= 2\n","    train_df, test_df, val_df = None, None, None\n","\n","    train_size = int(props[0] * len(df))\n","    val_size =  train_size + int(props[1] * len(df))\n","    test_size =val_size + int(props[2] * len(df)) \n","    train_df = df.iloc[0:train_size]\n","    val_df = df.iloc[train_size:val_size]\n","    test_df = df.iloc[val_size:test_size]\n","    \n","    return train_df, val_df, test_df"],"id":"_4lpUGTQzFFI","execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"u04VW4ugzFFI","executionInfo":{"status":"ok","timestamp":1638628875993,"user_tz":300,"elapsed":3,"user":{"displayName":"Andrés Langoyo Martín","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12914381003507187562"}}},"source":["import gensim.downloader as api\n","\n","def download_embeddings(fasttetxt):\n","    # https://fasttext.cc/docs/en/english-vectors.html\n","    if fasttetxt:\n","      wv = api.load(\"fasttext-wiki-news-subwords-300\")\n","    else:\n","      \n","      wv = api.load(\"word2vec-google-news-300\")\n","      print(\"\\nLoading complete!\\n\" +\n","            \"Vocabulary size: {}\".format(len(wv.vocab)))\n","    return wv\n"],"id":"u04VW4ugzFFI","execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"23WpDX0rzFFJ","executionInfo":{"status":"ok","timestamp":1638628888474,"user_tz":300,"elapsed":12411,"user":{"displayName":"Andrés Langoyo Martín","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12914381003507187562"}},"outputId":"effb163f-e645-4540-93e4-4fc6215cf880"},"source":["# Opening and preprocessing input file\n","import gensim.models\n","import pandas as pd\n","import nltk\n","nltk.download('punkt')\n","from tqdm import tqdm\n","from src.preprocess import clean_text\n","\n","data = pd.read_csv('train.csv', quotechar='\"')\n","data.sample(frac=1)\n","\n","\n","# to convert authors into numbers\n","author_to_number = {\n","    'EAP': 0,\n","    'HPL': 1,\n","    'MWS': 2\n","    \n","}\n","\n","# lowercase, removing punctuation and tookenize sentences. Converting labels to int\n","training_text = \"\"\n","for i in range(len(data)):\n","\n","    data['text'][i] = nltk.word_tokenize(regex.sub(r'[^\\w\\s]', '',data['text'][i].lower()))\n","    data['author'][i] = author_to_number[data['author'][i]]\n","\n","print(data[0:10])\n","print(len(data))\n","\n","from src.dataset import *\n","\n","# Splitting dataset and generating vocab\n","train_df, val_df, test_df = split_train_val_test(data)\n","train_vocab, reversed_vocab = generate_vocab_map(train_df)\n","\n"],"id":"23WpDX0rzFFJ","execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","        id                                               text author\n","0  id26305  [this, process, however, afforded, me, no, mea...      0\n","1  id17569  [it, never, once, occurred, to, me, that, the,...      1\n","2  id11008  [in, his, left, hand, was, a, gold, snuff, box...      0\n","3  id27763  [how, lovely, is, spring, as, we, looked, from...      2\n","4  id12958  [finding, nothing, else, not, even, gold, the,...      1\n","5  id22965  [a, youth, passed, in, solitude, my, best, yea...      2\n","6  id09674  [the, astronomer, perhaps, at, this, point, to...      0\n","7  id13515  [the, surcingle, hung, in, ribands, from, my, ...      0\n","8  id19322  [i, knew, that, you, could, not, say, to, your...      0\n","9  id00912  [i, confess, that, neither, the, structure, of...      2\n","19579\n"]}]},{"cell_type":"code","metadata":{"id":"NTDiU36-zFFK","executionInfo":{"status":"ok","timestamp":1638628888474,"user_tz":300,"elapsed":4,"user":{"displayName":"Andrés Langoyo Martín","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12914381003507187562"}}},"source":["DOWNLOAD = False\n","# Use fastext or word2vec\n","FASTTEXT = False\n","WINDOW_SIZE = 7\n","\n","EMBEDDING_DIM = 300\n","HIDDEN_DIM = 512\n","NUM_LAYERS = 2\n","BIDIRECTIONAL = True\n"],"id":"NTDiU36-zFFK","execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"kaqaR8CNzFFK","executionInfo":{"status":"ok","timestamp":1638628894140,"user_tz":300,"elapsed":5669,"user":{"displayName":"Andrés Langoyo Martín","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12914381003507187562"}}},"source":["# Downloading or generating word2vec embeddings\n","\n","if DOWNLOAD:\n","    model = download_embeddings(FASTTEXT)\n","else:\n","    if FASTTEXT:\n","        model = gensim.models.FastText(sentences=train_df['text'], size=EMBEDDING_DIM, window=WINDOW_SIZE)\n","    else:\n","        model = gensim.models.Word2Vec(sentences=train_df['text'], size=EMBEDDING_DIM, window=WINDOW_SIZE)\n","                        "],"id":"kaqaR8CNzFFK","execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"fVz5UQwTzFFL","executionInfo":{"status":"ok","timestamp":1638628894143,"user_tz":300,"elapsed":25,"user":{"displayName":"Andrés Langoyo Martín","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12914381003507187562"}}},"source":["from src.dataset import HeadlineDataset\n","from torch.utils.data import RandomSampler\n","\n","train_dataset = HeadlineDataset(train_vocab, train_df,model.wv, FASTTEXT)\n","val_dataset = HeadlineDataset(train_vocab, val_df,model.wv, FASTTEXT)\n","test_dataset = HeadlineDataset(train_vocab, test_df,model.wv, FASTTEXT)\n","\n","# Now that we're wrapping our dataframes in PyTorch datsets, we can make use of PyTorch Random Samplers.\n","train_sampler = RandomSampler(train_dataset)\n","val_sampler = RandomSampler(val_dataset)\n","test_sampler = RandomSampler(test_dataset)"],"id":"fVz5UQwTzFFL","execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gnfaq2kpzFFM","executionInfo":{"status":"ok","timestamp":1638628894353,"user_tz":300,"elapsed":228,"user":{"displayName":"Andrés Langoyo Martín","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12914381003507187562"}},"outputId":"6f0bffef-7e20-4b7c-f01d-1673d1b4213f"},"source":["from torch.utils.data import DataLoader\n","from src.dataset import collate_fn\n","BATCH_SIZE = 16\n","train_iterator = DataLoader(train_dataset, batch_size=BATCH_SIZE, sampler=train_sampler, collate_fn=collate_fn)\n","val_iterator = DataLoader(val_dataset, batch_size=BATCH_SIZE, sampler=val_sampler, collate_fn=collate_fn)\n","test_iterator = DataLoader(test_dataset, batch_size=BATCH_SIZE, sampler=test_sampler, collate_fn=collate_fn)\n","\n","for x, y in test_iterator:\n","    print(x,y)\n","    break"],"id":"gnfaq2kpzFFM","execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[[ 0.1818,  0.0874,  0.2061,  ...,  0.0344, -0.0481,  0.2806],\n","         [-0.0228,  0.0701,  0.0186,  ...,  0.0212,  0.0953, -0.0567],\n","         [-0.0205,  0.0545,  0.2630,  ...,  0.0011,  0.0663, -0.2337],\n","         ...,\n","         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n","\n","        [[-0.2800,  0.6788, -0.3693,  ...,  0.5671,  0.5093, -0.6363],\n","         [-0.0908,  0.2378, -0.0926,  ...,  0.0825,  0.3302, -0.1770],\n","         [-0.1122,  0.0618, -0.0596,  ..., -0.0296,  0.2521, -0.0659],\n","         ...,\n","         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n","\n","        [[ 0.2540,  0.8050, -0.4784,  ...,  0.2070,  0.9057, -0.2696],\n","         [ 0.4839,  0.3876, -0.2049,  ..., -0.2731,  0.6531,  0.5798],\n","         [-0.3847,  0.4944, -0.2404,  ...,  0.6422,  0.5279, -0.6186],\n","         ...,\n","         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n","\n","        ...,\n","\n","        [[ 0.1818,  0.0874,  0.2061,  ...,  0.0344, -0.0481,  0.2806],\n","         [-0.0141,  0.0512,  0.0093,  ..., -0.0078,  0.0646, -0.0404],\n","         [-0.0205,  0.0545,  0.2630,  ...,  0.0011,  0.0663, -0.2337],\n","         ...,\n","         [-0.0159,  0.1750,  0.0461,  ...,  0.0265,  0.2373, -0.0812],\n","         [-0.1298,  0.2819, -0.0408,  ...,  0.1135,  0.2456, -0.2442],\n","         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n","\n","        [[-0.3598,  0.2179,  0.1936,  ...,  0.2370,  0.3619, -0.2388],\n","         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","         [-0.2065, -0.1548, -0.3412,  ..., -0.1615,  0.2734,  0.2091],\n","         ...,\n","         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n","\n","        [[-0.1096,  0.1634,  0.1357,  ...,  0.1126,  0.1237, -0.1360],\n","         [-0.0009,  0.1464,  0.0591,  ..., -0.0641,  0.1890, -0.1261],\n","         [ 0.0666,  0.5555, -0.2607,  ...,  0.1813,  0.4370, -0.1339],\n","         ...,\n","         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]]) tensor([0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 2, 2, 2, 2, 0, 1])\n"]},{"output_type":"stream","name":"stderr","text":["/content/src/dataset.py:161: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n","  tokenized_word_tensor = torch.Tensor(tmp)\n"]}]},{"cell_type":"markdown","metadata":{"id":"FvFz28NgzFFM"},"source":["### Modeling"],"id":"FvFz28NgzFFM"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E54k4vH-zFFO","executionInfo":{"status":"ok","timestamp":1638628903364,"user_tz":300,"elapsed":9013,"user":{"displayName":"Andrés Langoyo Martín","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12914381003507187562"}},"outputId":"c51574b4-cb8b-40fb-93fb-eeec38eb14da"},"source":["from src.models import ClassificationModel\n","\n","model = ClassificationModel(len(train_vocab),embedding_dim=EMBEDDING_DIM,hidden_dim = HIDDEN_DIM,num_layers = NUM_LAYERS,bidirectional = BIDIRECTIONAL)\n","\n","model.to(device)"],"id":"E54k4vH-zFFO","execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["ClassificationModel(\n","  (LSTM): LSTM(300, 512, num_layers=2, batch_first=True, bidirectional=True)\n","  (linear): Linear(in_features=1024, out_features=3, bias=True)\n","  (softmax): Softmax(dim=1)\n",")"]},"metadata":{},"execution_count":13}]},{"cell_type":"markdown","metadata":{"id":"PeoanbgVzFFO"},"source":["In the following cell, **instantiate the model with some hyperparameters, and select an appropriate loss function and optimizer.** \n","\n","Hint: we already use sigmoid in our model. What loss functions are availible for binary classification? Feel free to look at PyTorch docs for help!"],"id":"PeoanbgVzFFO"},{"cell_type":"code","metadata":{"id":"4rcqsWzHzFFP","executionInfo":{"status":"ok","timestamp":1638628903365,"user_tz":300,"elapsed":36,"user":{"displayName":"Andrés Langoyo Martín","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12914381003507187562"}}},"source":["from torch.optim import AdamW\n","\n","criterion, optimizer = torch.nn.CrossEntropyLoss(), torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)"],"id":"4rcqsWzHzFFP","execution_count":14,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kNFrnFAVzFFP"},"source":["### Part 3: Training and Evaluation [10 Points]\n","The final part of this HW involves training the model, and evaluating it at each epoch. **Fill out the train and test loops below.**"],"id":"kNFrnFAVzFFP"},{"cell_type":"code","metadata":{"id":"4WOY7B-azFFP","executionInfo":{"status":"ok","timestamp":1638628903365,"user_tz":300,"elapsed":31,"user":{"displayName":"Andrés Langoyo Martín","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12914381003507187562"}}},"source":["# returns the total loss calculated from criterion\n","def train_loop(model, criterion, iterator):\n","    model.train()\n","    total_loss = 0\n","    \n","    for x, y in tqdm(iterator):\n","        x = x.to(device)\n","        y = y.to(device)\n","        optimizer.zero_grad()\n","\n","        prediction = model(x)\n","        prediction = torch.squeeze(prediction)\n","        # y = y.round()\n","        # y = y.long()\n","        \n","\n"," \n","        loss = criterion(prediction,y)\n","        total_loss += loss.item()\n","        loss.backward()\n","        optimizer.step()\n","\n","    return total_loss\n","\n","# returns:\n","# - true: a Python boolean array of all the ground truth values \n","#         taken from the dataset iterator\n","# - pred: a Python boolean array of all model predictions. \n","def val_loop(model, criterion, iterator):\n","    true, pred = [], []\n","    for x, y in tqdm(iterator):\n","        x = x.to(device)\n","        y = y.to(device)\n","    \n","        preds = model(x)\n","        preds.to(device)\n","        preds = torch.squeeze(preds)\n","        for i_batch in range(len(y)):\n","            true.append(y[i_batch])\n","            pred.append(torch.argmax(preds[i_batch]))\n","            \n","    return true, pred\n"],"id":"4WOY7B-azFFP","execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RwyLMPAwzFFQ","executionInfo":{"status":"ok","timestamp":1638628908320,"user_tz":300,"elapsed":4983,"user":{"displayName":"Andrés Langoyo Martín","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12914381003507187562"}},"outputId":"03c68c77-34f0-44bd-e418-c60e9bea55fd"},"source":["# To test your eval implementation, let's see how well the untrained model does on our dev dataset.\n","# It should do pretty poorly.\n","from src.eval_utils import binary_macro_f1, accuracy\n","true, pred = val_loop(model, criterion, val_iterator)\n","# print(binary_macro_f1(true, pred))\n","# print(accuracy(true, pred))\n"],"id":"RwyLMPAwzFFQ","execution_count":16,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 123/123 [00:05<00:00, 23.99it/s]\n"]}]},{"cell_type":"markdown","metadata":{"id":"klS-hqCezFFQ"},"source":["### Actually training the model"],"id":"klS-hqCezFFQ"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PS8EQy8SzFFQ","executionInfo":{"status":"ok","timestamp":1638630023007,"user_tz":300,"elapsed":836401,"user":{"displayName":"Andrés Langoyo Martín","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12914381003507187562"}},"outputId":"6aafee47-4bc7-4561-fd4d-d31cf95ce3c7"},"source":["TOTAL_EPOCHS = 7\n","for epoch in range(TOTAL_EPOCHS):\n","    train_loss = train_loop(model, criterion, train_iterator)\n","    true, pred = val_loop(model, criterion, val_iterator)\n","    print(f\"EPOCH: {epoch}\")\n","    print(f\"TRAIN LOSS: {train_loss}\")\n","    print(f\"VAL F-1: {binary_macro_f1(true, pred)}\")\n","    print(f\"VAL ACC: {accuracy(true, pred)}\")\n"],"id":"PS8EQy8SzFFQ","execution_count":17,"outputs":[{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 979/979 [02:32<00:00,  6.44it/s]\n","100%|██████████| 123/123 [00:04<00:00, 24.70it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["EPOCH: 0\n","TRAIN LOSS: 1042.6977465748787\n","VAL F-1: 0.4205877849157152\n","VAL ACC: 0.5002554931016863\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 979/979 [02:32<00:00,  6.40it/s]\n","100%|██████████| 123/123 [00:04<00:00, 25.02it/s]\n"]},{"output_type":"stream","name":"stdout","text":["EPOCH: 1\n","TRAIN LOSS: 955.4998814463615\n","VAL F-1: 0.5624099343683286\n","VAL ACC: 0.57230454777721\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 979/979 [02:34<00:00,  6.33it/s]\n","100%|██████████| 123/123 [00:04<00:00, 24.77it/s]\n"]},{"output_type":"stream","name":"stdout","text":["EPOCH: 2\n","TRAIN LOSS: 911.180206656456\n","VAL F-1: 0.5329414145996849\n","VAL ACC: 0.5717935615738375\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 979/979 [02:34<00:00,  6.35it/s]\n","100%|██████████| 123/123 [00:04<00:00, 25.14it/s]\n"]},{"output_type":"stream","name":"stdout","text":["EPOCH: 3\n","TRAIN LOSS: 892.4298995733261\n","VAL F-1: 0.5665024608254311\n","VAL ACC: 0.5743484925907001\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 979/979 [02:33<00:00,  6.39it/s]\n","100%|██████████| 123/123 [00:04<00:00, 24.98it/s]\n"]},{"output_type":"stream","name":"stdout","text":["EPOCH: 4\n","TRAIN LOSS: 871.6088212430477\n","VAL F-1: 0.6126606680108503\n","VAL ACC: 0.6177823198773633\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 979/979 [02:32<00:00,  6.40it/s]\n","100%|██████████| 123/123 [00:04<00:00, 24.80it/s]\n"]},{"output_type":"stream","name":"stdout","text":["EPOCH: 5\n","TRAIN LOSS: 860.8205380737782\n","VAL F-1: 0.5866773855015739\n","VAL ACC: 0.6065406234031682\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 979/979 [02:33<00:00,  6.36it/s]\n","100%|██████████| 123/123 [00:04<00:00, 24.94it/s]\n"]},{"output_type":"stream","name":"stdout","text":["EPOCH: 6\n","TRAIN LOSS: 843.1460570693016\n","VAL F-1: 0.6233450611185714\n","VAL ACC: 0.6249361267245784\n"]}]},{"cell_type":"markdown","metadata":{"id":"ECkuloBSzFFR"},"source":["We can also look at the models performance on the held-out test set, using the same val_loop we wrote earlier."],"id":"ECkuloBSzFFR"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E9S33DF4zFFR","executionInfo":{"status":"ok","timestamp":1638630028923,"user_tz":300,"elapsed":5923,"user":{"displayName":"Andrés Langoyo Martín","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12914381003507187562"}},"outputId":"d40de8b1-476a-47b3-a7cd-3697c51bfa3d"},"source":["true, pred = val_loop(model, criterion, test_iterator)\n","print(f\"TEST F-1: {binary_macro_f1(true, pred)}\")\n","print(f\"TEST ACC: {accuracy(true, pred)}\")"],"id":"E9S33DF4zFFR","execution_count":18,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 123/123 [00:05<00:00, 24.03it/s]\n"]},{"output_type":"stream","name":"stdout","text":["TEST F-1: 0.6087461232474088\n","TEST ACC: 0.6090955544200306\n"]}]}]}