{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Part 0. Google Colab Setup"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Hopefully you're looking at this notebook in Colab! \n",
    "1. First, make a copy of this notebook to your local drive, so you can edit it. \n",
    "2. Go ahead and upload the OnionOrNot.csv file from the [assignment zip](https://www.cc.gatech.edu/classes/AY2022/cs4650_fall/programming/h2_torch.zip) in the files panel on the left.\n",
    "3. Right click in the files panel, and select 'Create New Folder' - call this folder src\n",
    "4. Upload all the files in the src/ folder from the [assignment zip](https://www.cc.gatech.edu/classes/AY2022/cs4650_fall/programming/h2_torch.zip) to the src/ folder on colab.\n",
    "\n",
    "***NOTE: REMEMBER TO REGULARLY REDOWNLOAD ALL THE FILES IN SRC FROM COLAB.*** \n",
    "\n",
    "***IF YOU EDIT THE FILES IN COLAB, AND YOU DO NOT REDOWNLOAD THEM, YOU WILL LOSE YOUR WORK!***\n",
    "\n",
    "If you want GPU's, you can always change your instance type to GPU directly in Colab."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Part 1. Loading and Preprocessing Data [10 points]\n",
    "The following cell loads the OnionOrNot dataset, and tokenizes each data item"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# DO NOT MODIFY #\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "RANDOM_SEED = 42\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "# this is how we select a GPU if it's avalible on your computer.\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import pandas as pd\n",
    "from src.preprocess import clean_text \n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "\n",
    "nltk.download('punkt')\n",
    "df = pd.read_csv(\"OnionOrNot.csv\")\n",
    "df[\"tokenized\"] = df[\"text\"].apply(lambda x: nltk.word_tokenize(clean_text(x.lower())))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package punkt to /home/andre/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here's what the dataset looks like. You can index into specific rows with pandas, and try to guess some of these yourself :)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "df.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Entire Facebook Staff Laughs As Man Tightens P...</td>\n",
       "      <td>1</td>\n",
       "      <td>[entire, facebook, staff, laughs, as, man, tig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Muslim Woman Denied Soda Can for Fear She Coul...</td>\n",
       "      <td>0</td>\n",
       "      <td>[muslim, woman, denied, soda, can, for, fear, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bold Move: Hulu Has Announced That They’re Gon...</td>\n",
       "      <td>1</td>\n",
       "      <td>[bold, move, :, hulu, has, announced, that, th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Despondent Jeff Bezos Realizes He’ll Have To W...</td>\n",
       "      <td>1</td>\n",
       "      <td>[despondent, jeff, bezos, realizes, he, ’, ll,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>For men looking for great single women, online...</td>\n",
       "      <td>1</td>\n",
       "      <td>[for, men, looking, for, great, single, women,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label  \\\n",
       "0  Entire Facebook Staff Laughs As Man Tightens P...      1   \n",
       "1  Muslim Woman Denied Soda Can for Fear She Coul...      0   \n",
       "2  Bold Move: Hulu Has Announced That They’re Gon...      1   \n",
       "3  Despondent Jeff Bezos Realizes He’ll Have To W...      1   \n",
       "4  For men looking for great single women, online...      1   \n",
       "\n",
       "                                           tokenized  \n",
       "0  [entire, facebook, staff, laughs, as, man, tig...  \n",
       "1  [muslim, woman, denied, soda, can, for, fear, ...  \n",
       "2  [bold, move, :, hulu, has, announced, that, th...  \n",
       "3  [despondent, jeff, bezos, realizes, he, ’, ll,...  \n",
       "4  [for, men, looking, for, great, single, women,...  "
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "df.iloc[42]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "text         Customers continued to wait at drive-thru even...\n",
       "label                                                        0\n",
       "tokenized    [customers, continued, to, wait, at, drive-thr...\n",
       "Name: 42, dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now that we've loaded this dataset, we need to split the data into train, validation, and test sets. We also need to create a vocab map for words in our Onion dataset, which will map tokens to numbers. This will be useful later, since torch models can only use tensors of sequences of numbers as inputs. **Go to src/dataset.py, and fill out split_train_val_test, generate_vocab_map**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "## TODO: complete these methods in src/dataset.py\n",
    "from src.dataset import split_train_val_test, generate_vocab_map\n",
    "df = df.sample(frac=1)\n",
    "\n",
    "train_df, val_df, test_df = split_train_val_test(df, props=[.8, .1, .1])\n",
    "train_vocab, reverse_vocab = generate_vocab_map(train_df)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# this line of code will help test your implementation\n",
    "(len(train_df) / len(df)), (len(val_df) / len(df)), (len(test_df) / len(df))"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(0.8, 0.1, 0.1)"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "PyTorch has custom Datset Classes that have very useful extentions. **Go to src/dataset.py, and fill out the HeadlineDataset class.** Refer to PyTorch documentation on Dataset Classes for help."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "from src.dataset import HeadlineDataset\n",
    "from torch.utils.data import RandomSampler\n",
    "#print(train_df)\n",
    "\n",
    "train_dataset = HeadlineDataset(train_vocab, train_df)\n",
    "val_dataset = HeadlineDataset(train_vocab, val_df)\n",
    "test_dataset = HeadlineDataset(train_vocab, test_df)\n",
    "\n",
    "# Now that we're wrapping our dataframes in PyTorch datsets, we can make use of PyTorch Random Samplers.\n",
    "train_sampler = RandomSampler(train_dataset)\n",
    "val_sampler = RandomSampler(val_dataset)\n",
    "test_sampler = RandomSampler(test_dataset)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can now use PyTorch DataLoaders to batch our data for us. **Go to src/dataset.py, and fill out collate_fn.** Refer to PyTorch documentation on Dataloaders for help."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from src.dataset import collate_fn\n",
    "BATCH_SIZE = 16\n",
    "train_iterator = DataLoader(train_dataset, batch_size=BATCH_SIZE, sampler=train_sampler, collate_fn=collate_fn)\n",
    "val_iterator = DataLoader(val_dataset, batch_size=BATCH_SIZE, sampler=val_sampler, collate_fn=collate_fn)\n",
    "test_iterator = DataLoader(test_dataset, batch_size=BATCH_SIZE, sampler=test_sampler, collate_fn=collate_fn)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# # Use this to test your collate_fn implementation.\n",
    "\n",
    "# # You can look at the shapes of x and y or put print \n",
    "# # statements in collate_fn while running this snippet\n",
    "\n",
    "for x, y in test_iterator:\n",
    "    print(x,y)\n",
    "    break\n",
    "test_iterator = DataLoader(test_dataset, batch_size=BATCH_SIZE, sampler=test_sampler, collate_fn=collate_fn)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[ 174, 2903, 9462,    1,  321,  563,   36, 2201,    1, 1844, 2854,   12,\n",
      "          223,    1,   33, 7472,    0,    0,    0,    0,    0],\n",
      "        [  11, 1085,  828, 6476,  411,    1,   36,   37,   51,  158, 3298,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [   1, 2698, 8440,   33,  321,   33,  274,   52,    1,   41,  617, 4959,\n",
      "           12, 1016,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [  25,  882,  289,  552,  301,   61,  181,  593,  289, 2774, 2775,   36,\n",
      "         4604, 5445,   36,  113,    1,    1, 1094,   52,    3],\n",
      "        [3472, 4372, 1331,    1, 2430,   52,   57, 8872,    1,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [4223,  120,   15,  431,    1,  217, 5502,   24,    1,  608,   52,    1,\n",
      "          925,    0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [3019,   61, 3210, 2774,    1, 6016, 6635, 4447,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [3123, 4183, 3475,  217, 5632, 8439,   52, 8594, 5089,   21,  605,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [3283,  411,   61, 2461,  158,  476, 3157,  939, 3310, 2127, 6876,  411,\n",
      "           33,  343,  327,   21, 7367,  301,    0,    0,    0],\n",
      "        [  68,  147, 2389,   33, 7045,  951, 8886,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [1641, 5892,   61, 5897,  976, 8704,  133,  263,    1,    1,   56,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [   1, 2710,    1,  165,    1,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [  11,   21, 1021, 6181,  619,  148,  149, 3427,  128, 3879,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [5753, 4651,  321,    1,    1,   21, 1651,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [7449, 6726,  770, 5105, 3296,   36,   81,  783,    1,    1, 6870,  783,\n",
      "          326, 4127,   30,    0,    0,    0,    0,    0,    0],\n",
      "        [1280,   36, 6891, 3256,    1, 1752, 1010,   24, 2486, 4671,   30,   31,\n",
      "         2121,  976,    1, 5917,    1,    0,    0,    0,    0]]) tensor([1., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1.])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Part 2: Modeling [10 pts]\n",
    "Let's move to modeling, now that we have dataset iterators that batch our data for us. **Go to src/model.py, and follow the instructions in the file to create a basic neural network. Then, create your model using the class, and define hyperparameters.** "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "from src.models import ClassificationModel\n",
    "model = None\n",
    "### YOUR CODE GOES HERE (1 line of code) ###\n",
    "model = ClassificationModel(len(train_vocab),embedding_dim=32,hidden_dim = 32,num_layers = 1,bidirectional = True)\n",
    "\n",
    "# model.to(device)\n",
    "# # \n",
    "### YOUR CODE ENDS HERE ###"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the following cell, **instantiate the model with some hyperparameters, and select an appropriate loss function and optimizer.** \n",
    "\n",
    "Hint: we already use sigmoid in our model. What loss functions are availible for binary classification? Feel free to look at PyTorch docs for help!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "criterion, optimizer = None, None\n",
    "### YOUR CODE GOES HERE ###\n",
    "criterion, optimizer = torch.nn.BCEWithLogitsLoss(), torch.optim.AdamW(model.parameters(), lr=0.01)\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.9)\n",
    "\n",
    "### YOUR CODE ENDS HERE ###"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Part 3: Training and Evaluation [10 Points]\n",
    "The final part of this HW involves training the model, and evaluating it at each epoch. **Fill out the train and test loops below.**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "# returns the total loss calculated from criterion\n",
    "def train_loop(model, criterion, iterator):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for x, y in tqdm(iterator):\n",
    "        optimizer.zero_grad()\n",
    "        # x = x.to(device)\n",
    "        # y = y.to(device)\n",
    "        ### YOUR CODE STARTS HERE (~6 lines of code) ###\n",
    "        prediction = model(x)\n",
    "        prediction = torch.squeeze(prediction,0)\n",
    "        y = y.unsqueeze(1)\n",
    "        y = y.round()\n",
    "\n",
    " \n",
    "        loss = criterion(prediction,y)\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    # scheduler.step()\n",
    "        ### YOUR CODE ENDS HERE ###\n",
    "    return total_loss\n",
    "\n",
    "# returns:\n",
    "# - true: a Python boolean array of all the ground truth values \n",
    "#         taken from the dataset iterator\n",
    "# - pred: a Python boolean array of all model predictions. \n",
    "def val_loop(model, criterion, iterator):\n",
    "    true, pred = [], []\n",
    "    ### YOUR CODE STARTS HERE (~8 lines of code) ###\n",
    "    for x, y in tqdm(iterator):\n",
    "        # x = x.to(device)\n",
    "        # y = y.to(device)\n",
    "        # print(\"x\",x)\n",
    "        # print(\"y\",y)  \n",
    "    \n",
    "        preds = model(x)\n",
    "        preds = torch.flatten(preds)\n",
    "        for i_batch in range(len(y)):\n",
    "            true.append(y[i_batch])\n",
    "            pred.append(torch.round(preds[i_batch]))\n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "    ### YOUR CODE ENDS HERE ###\n",
    "    return true, pred\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We also need evaluation metrics that tell us how well our model is doing on the validation set at each epoch. **Complete the functions in src/eval.py.**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "# To test your eval implementation, let's see how well the untrained model does on our dev dataset.\n",
    "# It should do pretty poorly.\n",
    "from src.eval_utils import binary_macro_f1, accuracy\n",
    "true, pred = val_loop(model, criterion, val_iterator)\n",
    "print(binary_macro_f1(true, pred))\n",
    "print(accuracy(true, pred))\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 150/150 [00:02<00:00, 69.12it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.4216917201986171\n",
      "0.42791666666666667\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Part 4: Actually training the model [1 point]\n",
    "Watch your model train :D You should be able to achieve a validation F-1 score of at least .8 if everything went correctly. **Feel free to adjust the number of epochs to prevent overfitting or underfitting.**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "TOTAL_EPOCHS = 7\n",
    "for epoch in range(TOTAL_EPOCHS):\n",
    "    train_loss = train_loop(model, criterion, train_iterator)\n",
    "    true, pred = val_loop(model, criterion, val_iterator)\n",
    "    print(f\"EPOCH: {epoch}\")\n",
    "    print(f\"TRAIN LOSS: {train_loss}\")\n",
    "    print(f\"VAL F-1: {binary_macro_f1(true, pred)}\")\n",
    "    print(f\"VAL ACC: {accuracy(true, pred)}\")\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 1200/1200 [01:35<00:00, 12.61it/s]\n",
      "100%|██████████| 150/150 [00:02<00:00, 73.79it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "EPOCH: 0\n",
      "TRAIN LOSS: 775.4171956777573\n",
      "VAL F-1: 0.7974621640060507\n",
      "VAL ACC: 0.8170833333333334\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 1200/1200 [01:34<00:00, 12.68it/s]\n",
      "100%|██████████| 150/150 [00:01<00:00, 79.89it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "EPOCH: 1\n",
      "TRAIN LOSS: 723.8004207611084\n",
      "VAL F-1: 0.8273450802293716\n",
      "VAL ACC: 0.8391666666666666\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 1200/1200 [01:34<00:00, 12.65it/s]\n",
      "100%|██████████| 150/150 [00:01<00:00, 85.69it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "EPOCH: 2\n",
      "TRAIN LOSS: 706.5095884203911\n",
      "VAL F-1: 0.8122257420283887\n",
      "VAL ACC: 0.8304166666666667\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 1200/1200 [01:34<00:00, 12.72it/s]\n",
      "100%|██████████| 150/150 [00:01<00:00, 81.95it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "EPOCH: 3\n",
      "TRAIN LOSS: 698.9961122572422\n",
      "VAL F-1: 0.8362141298875652\n",
      "VAL ACC: 0.8475\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 1200/1200 [01:34<00:00, 12.67it/s]\n",
      "100%|██████████| 150/150 [00:01<00:00, 82.50it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "EPOCH: 4\n",
      "TRAIN LOSS: 693.1515847146511\n",
      "VAL F-1: 0.8448174961697538\n",
      "VAL ACC: 0.8525\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 1200/1200 [01:39<00:00, 12.03it/s]\n",
      "100%|██████████| 150/150 [00:01<00:00, 78.85it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "EPOCH: 5\n",
      "TRAIN LOSS: 689.8829775452614\n",
      "VAL F-1: 0.8387900771050092\n",
      "VAL ACC: 0.8495833333333334\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 1200/1200 [01:42<00:00, 11.65it/s]\n",
      "100%|██████████| 150/150 [00:01<00:00, 77.04it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "EPOCH: 6\n",
      "TRAIN LOSS: 686.4265978038311\n",
      "VAL F-1: 0.8449919692293513\n",
      "VAL ACC: 0.8545833333333334\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can also look at the models performance on the held-out test set, using the same val_loop we wrote earlier."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "true, pred = val_loop(model, criterion, test_iterator)\n",
    "print(f\"TEST F-1: {binary_macro_f1(true, pred)}\")\n",
    "print(f\"TEST ACC: {accuracy(true, pred)}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 150/150 [00:01<00:00, 77.23it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "TEST F-1: 0.8448861905182727\n",
      "TEST ACC: 0.8570833333333333\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Part 5: Analysis [5 points]\n",
    "Answer the following questions:\n",
    "#### 1. What happens to the vocab size as you change the cutoff in the cell below? Can you explain this in the context of [Zipf's Law](https://en.wikipedia.org/wiki/Zipf%27s_law)?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "tmp_vocab, _ = generate_vocab_map(train_df, cutoff = 0)\n",
    "print(len(tmp_vocab))\n",
    "\"\"\"\n",
    "The cutoff discriminates which words are going to be part of our vocabulary looking at the number of appearances in our training data.\n",
    "Setting it to a value of 1 means that all the words that appear more than 0 times enter in our vocabulary.\n",
    "The progression we experiment is:\n",
    "cutoff len\n",
    "------ ---\n",
    "1      13298 \n",
    "2       9540\n",
    "3       7612\n",
    "4       6340\n",
    "5       5476\n",
    "6       4825\n",
    "7       4296\n",
    "8       3870\n",
    "9       3590\n",
    "We see that the number of words in our vocabulary decreases in what it seems a logarithmic progression.\n",
    "Zipf's Law states that the frequency of any word is inversely proportional to the rank of the word in the freq table.\n",
    "That means the most frequent words are always at the top of the table.\n",
    "With the cutoff, what we are doing is removing the words with the less frequency. The most frequent words will have very different frequencies from one\n",
    "another but the words at the bottom of the table will share their frequencies and they will be many. By increasing the cutoff, we remove each time less and\n",
    "less words as there will be less words that share their frequencies.\n",
    "\"\"\""
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "25387\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\"\\nThe cutoff discriminates which words are going to be part of our vocabulary looking at the number of appearances in our training data.\\nSetting it to a value of 1 means that all the words that appear more than 0 times enter in our vocabulary.\\nThe progression we experiment is:\\ncutoff len\\n------ ---\\n1      13298 \\n2       9540\\n3       7612\\n4       6340\\n5       5476\\n6       4825\\n7       4296\\n8       3870\\n9       3590\\nWe see that the number of words in our vocabulary decreases in what it seems a logarithmic progression.\\nZipf's Law states that the frequency of any word is inversely proportional to the rank of the word in the freq table.\\nThat means the most frequent words are always at the top of the table.\\nWith the cutoff, what we are doing is removing the words with the less frequency. The most frequent words will have very different frequencies from one\\nanother but the words at the bottom of the table will share their frequencies and they will be many. By increasing the cutoff, we remove each time less and\\nless words as there will be less words that share their frequencies.\\n\""
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2. Can you describe what cases the model is getting wrong in the witheld test-set? \n",
    "\n",
    "To do this, you'll need to create a new val_train_loop (``val_train_loop_incorrect``) so it returns incorrect sequences **and** you'll need to decode these sequences back into words. \n",
    "Thankfully, you've already created a map that can convert encoded sequences back to regular English: you will find the ``reverse_vocab`` variable useful.\n",
    "\n",
    "```\n",
    "# i.e. using a reversed map of {\"hi\": 2, \"hello\": 3, \"UNK\": 0}\n",
    "# we can turn [1, 2, 0] into this => [\"hi\", \"hello\", \"UNK\"]\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "# Implement this however you like! It should look very similar to val_loop.\n",
    "# Pass the test_iterator through this function to look at errors in the test set.\n",
    "def val_train_loop_incorrect(model, iterator):\n",
    "    for x, y in tqdm(iterator):\n",
    "        # x.to(device)\n",
    "        # y.to(device)\n",
    "\n",
    "        errors = []\n",
    "        \n",
    "        preds = model(x)\n",
    "        preds = torch.flatten(preds)\n",
    "        \n",
    "        for i_batch in range(len(y)):\n",
    "            if y[i_batch] != preds[i_batch]:\n",
    "                sentence = []\n",
    "                for word in range(len(x[i_batch])):\n",
    "                    sentence.append(reverse_vocab[x[i_batch][word].item()])\n",
    "                errors.append(sentence)\n",
    "        return errors\n",
    "                \n",
    "                \n",
    "    \n",
    "errors = val_train_loop_incorrect(model,test_iterator)\n",
    "for sentence in errors:\n",
    "    print(sentence)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  0%|          | 0/150 [00:00<?, ?it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['us', 'airways', 'tweets', 'UNK', 'UNK', 'UNK', 'at', 'angry', 'customer', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['UNK', 'replaces', 'UNK', 'coin', 'UNK', 'with', 'beer', 'taps', ',', 'UNK', 'winners', 'with', 'brew', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['beloved', 'honorary', 'cat', 'mayor', 'in', 'small', 'alaska', 'town', 'dies', 'at', '20', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['eagles', 'fans', 'finally', 'sober', 'enough', 'to', 'return', 'to', 'work', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['news', ':', 'finally', ':', 'the', 'indians', 'are', 'replacing', 'their', 'racist', 'mascot', 'chief', 'UNK', 'with', 'a', 'white', 'woman', 'wearing', 'a', 'native', 'american', 'halloween', 'costume', '']\n",
      "['these', 'brave', 'teens', 'went', 'UNK', 'for', '3', 'whole', 'days', 'and', 'miraculously', 'survived', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['lou', 'UNK', 'blames', 'stanford', '’', 's', 'loss', 'to', 'utah', 'on', 'UNK', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['mayor', 'rob', 'ford', 'calls', 'in', 'sick', 'on', 'bob', 'UNK', 'day', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['amsterdam', 'to', 'fly', 'rainbow', 'flag', 'for', 'russian', 'president', 'putin', '’', 's', 'visit', 'to', 'the', 'capital', '', '', '', '', '', '', '', '', '']\n",
      "['four', 'UNK', 'took', 'cocaine', 'thinking', 'it', 'was', 'UNK', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['for', 'the', 'first', 'time', 'in', 'saudi', 'arabia', ',', 'women', 'UNK', 'to', 'issue', 'UNK', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['boy', 'suspended', 'over', 'UNK', 'gun', 'seeks', 'to', 'clear', 'school', 'record', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['‘', 'nothing', 'would', 'surprise', 'me', 'at', 'this', 'point', ',', '’', 'says', 'man', 'who', 'will', 'be', 'shocked', 'by', '8', 'separate', 'news', 'items', 'today', '', '']\n",
      "['why', 'north', 'korea', \"'s\", 'capital', 'is', 'the', 'UNK', 'science', 'fiction', 'film', 'set', \"'\", '', '', '', '', '', '', '', '', '', '', '']\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "interpreter": {
   "hash": "fd4653ffb3619e38e0f162702933cb5a2e71428b78fc95dca1bdeccba0429964"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}