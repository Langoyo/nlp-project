{"cells":[{"cell_type":"code","execution_count":1,"id":"NqHPu2i_RIdi","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1688,"status":"ok","timestamp":1638977484511,"user":{"displayName":"Andres Langoyo","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16006428957903159935"},"user_tz":300},"id":"NqHPu2i_RIdi","outputId":"4035ef67-833b-4ad5-e68c-d769caaa1470"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 1)) (1.10.0+cu111)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 2)) (1.19.5)\n","Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 3)) (3.6.0)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 4)) (3.2.5)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 5)) (1.1.5)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 6)) (4.62.3)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->-r requirements.txt (line 1)) (3.10.0.2)\n","Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim->-r requirements.txt (line 3)) (1.4.1)\n","Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim->-r requirements.txt (line 3)) (5.2.1)\n","Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim->-r requirements.txt (line 3)) (1.15.0)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->-r requirements.txt (line 5)) (2.8.2)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->-r requirements.txt (line 5)) (2018.9)\n"]}],"source":["!pip install -r requirements.txt"]},{"cell_type":"code","execution_count":null,"id":"b424ed91","metadata":{},"outputs":[],"source":["do_train = False"]},{"cell_type":"code","execution_count":2,"id":"1yv5wo80zFFD","metadata":{"executionInfo":{"elapsed":1026,"status":"ok","timestamp":1638977485531,"user":{"displayName":"Andres Langoyo","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16006428957903159935"},"user_tz":300},"id":"1yv5wo80zFFD"},"outputs":[],"source":["import torch\n","import random\n","import numpy as np\n","import regex\n","\n","RANDOM_SEED = 42\n","torch.manual_seed(RANDOM_SEED)\n","random.seed(RANDOM_SEED)\n","np.random.seed(RANDOM_SEED)\n","\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"]},{"cell_type":"code","execution_count":3,"id":"_4lpUGTQzFFI","metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1638977485532,"user":{"displayName":"Andres Langoyo","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16006428957903159935"},"user_tz":300},"id":"_4lpUGTQzFFI"},"outputs":[],"source":["def split_train_val_test(df, props=[.9, .1]):\n","    train_df, val_df = None, None\n","    \n","    train_size = int(props[0] * len(df))\n","    val_size =  train_size + int(props[1] * len(df))\n","    train_df = df.iloc[0:train_size]\n","    val_df = df.iloc[train_size:]\n","    return train_df, val_df\n"]},{"cell_type":"code","execution_count":4,"id":"u04VW4ugzFFI","metadata":{"executionInfo":{"elapsed":510,"status":"ok","timestamp":1638977486040,"user":{"displayName":"Andres Langoyo","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16006428957903159935"},"user_tz":300},"id":"u04VW4ugzFFI"},"outputs":[],"source":["import gensim.downloader as api\n","\n","def download_embeddings(fasttetxt):\n","    # https://fasttext.cc/docs/en/english-vectors.html\n","    if fasttetxt:\n","      wv = api.load(\"fasttext-wiki-news-subwords-300\")\n","    else:\n","      \n","      wv = api.load(\"word2vec-google-news-300\")\n","      print(\"\\nLoading complete!\\n\" +\n","            \"Vocabulary size: {}\".format(len(wv.vocab)))\n","    return wv\n"]},{"cell_type":"code","execution_count":5,"id":"23WpDX0rzFFJ","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":241},"executionInfo":{"elapsed":17027,"status":"ok","timestamp":1638977503065,"user":{"displayName":"Andres Langoyo","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16006428957903159935"},"user_tz":300},"id":"23WpDX0rzFFJ","outputId":"a28056fe-12c4-48ff-9eb1-2f0bab953c9c"},"outputs":[{"name":"stdout","output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>text</th>\n","      <th>author</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>17613</th>\n","      <td>id08561</td>\n","      <td>[a, lamp, which, had, been, accidentally, left...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>17614</th>\n","      <td>id01432</td>\n","      <td>[i, gave, to, each, heroine, of, whom, i, read...</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>17615</th>\n","      <td>id22037</td>\n","      <td>[he, got, in, communication, with, dr, houghto...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>17616</th>\n","      <td>id22330</td>\n","      <td>[the, trees, of, the, frequent, forest, belts,...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>17617</th>\n","      <td>id26151</td>\n","      <td>[i, then, moved, forward, and, a, murmuring, s...</td>\n","      <td>2</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["            id                                               text author\n","17613  id08561  [a, lamp, which, had, been, accidentally, left...      0\n","17614  id01432  [i, gave, to, each, heroine, of, whom, i, read...      2\n","17615  id22037  [he, got, in, communication, with, dr, houghto...      1\n","17616  id22330  [the, trees, of, the, frequent, forest, belts,...      1\n","17617  id26151  [i, then, moved, forward, and, a, murmuring, s...      2"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["# Opening and preprocessing input file\n","import gensim.models\n","import pandas as pd\n","import nltk\n","nltk.download('punkt')\n","from tqdm import tqdm\n","from preprocess import clean_text\n","\n","data = pd.read_pickle('our_train.pkl')\n","test_df = pd.read_pickle('our_test.pkl')\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# to convert authors into numbers\n","author_to_number = {\n","    'EAP': 0,\n","    'HPL': 1,\n","    'MWS': 2\n","    \n","}\n","\n","# lowercase, removing punctuation and tookenize sentences. Converting labels to int\n","for i in range(len(data)):\n","    data['text'].iloc[i] = nltk.word_tokenize(regex.sub(r'[^\\w\\s]', '',data['text'].iloc[i].lower()))\n","    data['author'].iloc[i] = author_to_number[data['author'].iloc[i]]\n","data.sample(frac=1)\n","for i in range(len(test_df)):\n","    test_df['text'].iloc[i] = nltk.word_tokenize(regex.sub(r'[^\\w\\s]', '',test_df['text'].iloc[i].lower()))\n","    test_df['author'].iloc[i] = author_to_number[test_df['author'].iloc[i]]\n","test_df.sample(frac=1)\n","from dataset import *\n","# Splitting dataset and generating vocab\n","train_df, val_df = split_train_val_test(data)\n","train_vocab, reversed_vocab = generate_vocab_map(train_df)\n","\n","val_df.head()\n","test_df.head()"]},{"cell_type":"code","execution_count":6,"id":"NTDiU36-zFFK","metadata":{"executionInfo":{"elapsed":17,"status":"ok","timestamp":1638977503068,"user":{"displayName":"Andres Langoyo","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16006428957903159935"},"user_tz":300},"id":"NTDiU36-zFFK"},"outputs":[],"source":["# Use downloaded pretrained embeddings or train our own\n","DOWNLOAD = False\n","# Use fastext or word2vec\n","FASTTEXT = True\n","WINDOW_SIZE = 5\n","\n","EMBEDDING_DIM = 300\n","HIDDEN_DIM = 128\n","NUM_LAYERS = 1\n","BIDIRECTIONAL = True\n"]},{"cell_type":"code","execution_count":7,"id":"kaqaR8CNzFFK","metadata":{"executionInfo":{"elapsed":22168,"status":"ok","timestamp":1638977525222,"user":{"displayName":"Andres Langoyo","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16006428957903159935"},"user_tz":300},"id":"kaqaR8CNzFFK"},"outputs":[],"source":["# Downloading or generating word2vec embeddings\n","\n","if DOWNLOAD:\n","    model = download_embeddings(FASTTEXT)\n","else:\n","    if FASTTEXT:\n","        model = gensim.models.FastText(sentences=train_df['text'], size=EMBEDDING_DIM, window=WINDOW_SIZE)\n","    else:\n","        model = gensim.models.Word2Vec(sentences=train_df['text'], size=EMBEDDING_DIM, window=WINDOW_SIZE)\n","                        "]},{"cell_type":"code","execution_count":8,"id":"fVz5UQwTzFFL","metadata":{"executionInfo":{"elapsed":38,"status":"ok","timestamp":1638977525224,"user":{"displayName":"Andres Langoyo","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16006428957903159935"},"user_tz":300},"id":"fVz5UQwTzFFL"},"outputs":[],"source":["from dataset import HeadlineDataset\n","from torch.utils.data import RandomSampler\n","\n","train_dataset = HeadlineDataset(train_vocab, train_df,model.wv, FASTTEXT)\n","val_dataset = HeadlineDataset(train_vocab, val_df,model.wv, FASTTEXT)\n","test_dataset = HeadlineDataset(train_vocab, test_df,model.wv, FASTTEXT)\n","\n","# Pytorch random samplers\n","train_sampler = RandomSampler(train_dataset)\n","val_sampler = RandomSampler(val_dataset)\n","test_sampler = RandomSampler(test_dataset)"]},{"cell_type":"code","execution_count":9,"id":"gnfaq2kpzFFM","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":35,"status":"ok","timestamp":1638977525224,"user":{"displayName":"Andres Langoyo","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16006428957903159935"},"user_tz":300},"id":"gnfaq2kpzFFM","outputId":"9868033a-7787-4b55-966d-0c50893c0dd2"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[[ 0.0623,  0.1292, -0.1069,  ...,  0.0344, -0.0569, -0.1036],\n","         [ 0.0860,  0.1488, -0.1444,  ...,  0.0556, -0.0941, -0.1517],\n","         [ 0.1233,  0.1547, -0.1772,  ...,  0.0547, -0.2526, -0.0743],\n","         ...,\n","         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n","\n","        [[ 0.4638,  0.5383, -0.4286,  ..., -0.1889,  0.2087, -0.2740],\n","         [ 0.2355,  0.5322, -0.0566,  ..., -0.3584, -0.3478, -0.3276],\n","         [ 0.1502,  0.2941, -0.0822,  ...,  0.2190, -0.0168, -0.2309],\n","         ...,\n","         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n","\n","        [[ 0.2355,  0.5322, -0.0566,  ..., -0.3584, -0.3478, -0.3276],\n","         [-0.0380,  0.1748,  0.0051,  ...,  0.1136, -0.1471, -0.1668],\n","         [ 0.1610,  0.3964, -0.2172,  ..., -0.0551, -0.0395, -0.3966],\n","         ...,\n","         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n","\n","        ...,\n","\n","        [[ 0.2355,  0.5322, -0.0566,  ..., -0.3584, -0.3478, -0.3276],\n","         [ 0.0437,  0.2602, -0.0162,  ...,  0.1088,  0.0122, -0.2219],\n","         [ 0.1111,  0.0905, -0.2877,  ...,  0.1413, -0.0626, -0.1985],\n","         ...,\n","         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n","\n","        [[ 0.6565,  0.4932, -0.4188,  ..., -0.2903,  0.2714, -0.1381],\n","         [ 0.1610,  0.3964, -0.2172,  ..., -0.0551, -0.0395, -0.3966],\n","         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","         ...,\n","         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n","\n","        [[ 0.4638,  0.5383, -0.4286,  ..., -0.1889,  0.2087, -0.2740],\n","         [ 0.4010,  0.0748, -0.5143,  ...,  0.0151,  0.1835, -0.1518],\n","         [ 0.1610,  0.3964, -0.2172,  ..., -0.0551, -0.0395, -0.3966],\n","         ...,\n","         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]]) tensor([1, 1, 2, 1, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0])\n"]},{"name":"stderr","output_type":"stream","text":["/content/dataset.py:133: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n","  tokenized_word_tensor = torch.Tensor(tmp)\n"]}],"source":["from torch.utils.data import DataLoader\n","from dataset import collate_fn\n","BATCH_SIZE = 16\n","# Creating data iterators\n","train_iterator = DataLoader(train_dataset, batch_size=BATCH_SIZE, sampler=train_sampler, collate_fn=collate_fn)\n","val_iterator = DataLoader(val_dataset, batch_size=BATCH_SIZE, sampler=val_sampler, collate_fn=collate_fn)\n","test_iterator = DataLoader(test_dataset, batch_size=BATCH_SIZE, sampler=test_sampler, collate_fn=collate_fn)\n","\n","for x, y in test_iterator:\n","    print(x,y)\n","    break"]},{"cell_type":"markdown","id":"FvFz28NgzFFM","metadata":{"id":"FvFz28NgzFFM"},"source":["### Modeling"]},{"cell_type":"code","execution_count":10,"id":"E54k4vH-zFFO","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2462,"status":"ok","timestamp":1638977527658,"user":{"displayName":"Andres Langoyo","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16006428957903159935"},"user_tz":300},"id":"E54k4vH-zFFO","outputId":"3d7776ce-b9c3-4aab-a7e5-a6650b349f8a"},"outputs":[{"data":{"text/plain":["ClassificationModel(\n","  (LSTM): LSTM(300, 128, batch_first=True, bidirectional=True)\n","  (linear): Linear(in_features=256, out_features=3, bias=True)\n","  (softmax): Softmax(dim=1)\n",")"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["from models import ClassificationModel\n","\n","model = ClassificationModel(len(train_vocab),embedding_dim=EMBEDDING_DIM,hidden_dim = HIDDEN_DIM,num_layers = NUM_LAYERS,bidirectional = BIDIRECTIONAL)\n","\n","model.to(device)"]},{"cell_type":"code","execution_count":11,"id":"4rcqsWzHzFFP","metadata":{"executionInfo":{"elapsed":22,"status":"ok","timestamp":1638977527660,"user":{"displayName":"Andres Langoyo","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16006428957903159935"},"user_tz":300},"id":"4rcqsWzHzFFP"},"outputs":[],"source":["from torch.optim import AdamW\n","\n","criterion, optimizer = torch.nn.CrossEntropyLoss(), torch.optim.Adam(model.parameters(), lr=0.001)"]},{"cell_type":"markdown","id":"xm7rdLC2sCfY","metadata":{"id":"xm7rdLC2sCfY"},"source":["# Testing and Evaluation"]},{"cell_type":"code","execution_count":12,"id":"4WOY7B-azFFP","metadata":{"executionInfo":{"elapsed":21,"status":"ok","timestamp":1638977528598,"user":{"displayName":"Andres Langoyo","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16006428957903159935"},"user_tz":300},"id":"4WOY7B-azFFP"},"outputs":[],"source":["# returns the total loss calculated from criterion\n","def train_loop(model, criterion, iterator):\n","    model.train()\n","    total_loss = 0\n","    \n","    for x, y in tqdm(iterator):\n","        x = x.to(device)\n","        y = y.to(device)\n","        optimizer.zero_grad()\n","\n","        prediction = model(x)\n","        prediction = torch.squeeze(prediction)\n","        # y = y.round()\n","        y = y.long()\n","        \n","\n"," \n","        loss = criterion(prediction,y)\n","        total_loss += loss.item()\n","        loss.backward()\n","        optimizer.step()\n","\n","    return total_loss\n","\n","# returns:\n","# - true: a Python boolean array of all the ground truth values \n","#         taken from the dataset iterator\n","# - pred: a Python boolean array of all model predictions. \n","def val_loop(model, criterion, iterator):\n","    true, pred = [], []\n","    for x, y in tqdm(iterator):\n","        x = x.to(device)\n","        y = y.to(device)\n","    \n","        preds = model(x)\n","        preds.to(device)\n","        preds = torch.squeeze(preds)\n","        for i_batch in range(len(y)):\n","            true.append(y[i_batch])\n","            pred.append(torch.argmax(preds[i_batch]))\n","            \n","    return true, pred\n"]},{"cell_type":"code","execution_count":13,"id":"RwyLMPAwzFFQ","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4286,"status":"ok","timestamp":1638977532867,"user":{"displayName":"Andres Langoyo","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16006428957903159935"},"user_tz":300},"id":"RwyLMPAwzFFQ","outputId":"3c2edccd-c5a8-4249-a9bf-286a42e26b9c"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 111/111 [00:04<00:00, 27.49it/s]"]},{"name":"stdout","output_type":"stream","text":["0.1586828971089003\n","0.29284903518728717\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["# Initial testing\n","from sklearn.metrics import f1_score, accuracy_score\n","\n","from eval_utils import binary_macro_f1, accuracy\n","true, pred = val_loop(model, criterion, val_iterator)\n","true = [x.item() for x in true]\n","pred = [x.item() for x in pred]\n","print(f1_score(true, pred, average='weighted'))\n","print(accuracy_score(true, pred))\n"]},{"cell_type":"markdown","id":"klS-hqCezFFQ","metadata":{"id":"klS-hqCezFFQ"},"source":["### Training the model\n","Do not run this for testing"]},{"cell_type":"code","execution_count":null,"id":"PS8EQy8SzFFQ","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PS8EQy8SzFFQ","outputId":"b30e82d5-9765-4edf-da58-7c1ac7561319"},"outputs":[{"name":"stderr","output_type":"stream","text":[" 12%|█▏        | 115/991 [00:14<01:46,  8.20it/s]"]}],"source":["if do_train:\n","    TOTAL_EPOCHS = 7\n","    for epoch in range(TOTAL_EPOCHS):\n","        train_loss = train_loop(model, criterion, train_iterator)\n","        true, pred = val_loop(model, criterion, val_iterator)\n","        true = [x.item() for x in true]\n","        pred = [x.item() for x in pred]\n","        print(f\"EPOCH: {epoch}\")\n","        print(f\"TRAIN LOSS: {train_loss}\")\n","        print(f\"VAL F-1: {f1_score(true, pred, average='weighted')}\")\n","        print(f\"VAL ACC: {accuracy_score(true, pred)}\")\n","    file = open('no_downloaded_fasttext.model', 'w+')    \n","    torch.save(model.state_dict(), f'no_downloaded_fasttext.model')\n"]},{"cell_type":"code","execution_count":14,"id":"xZE3RnPw2jo6","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17,"status":"ok","timestamp":1638977532869,"user":{"displayName":"Andres Langoyo","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16006428957903159935"},"user_tz":300},"id":"xZE3RnPw2jo6","outputId":"803e46ac-1a88-4e9e-bc0f-800e94ec52e6"},"outputs":[{"data":{"text/plain":["<All keys matched successfully>"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["# Loading saved model\n","model.load_state_dict(torch.load('no_downloaded_fasttext.model', map_location=torch.device('cpu')))"]},{"cell_type":"code","execution_count":15,"id":"E9S33DF4zFFR","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4678,"status":"ok","timestamp":1638977537532,"user":{"displayName":"Andres Langoyo","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16006428957903159935"},"user_tz":300},"id":"E9S33DF4zFFR","outputId":"273ce83e-d413-4eb9-9cc5-6106725cd424"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 123/123 [00:04<00:00, 27.42it/s]\n"]},{"name":"stdout","output_type":"stream","text":["TEST F-1: 0.6065385313162226\n","TEST ACC: 0.6078331637843337\n"]}],"source":["# Testing results\n","true, pred = val_loop(model, criterion, test_iterator)\n","true = [x.item() for x in true]\n","pred = [x.item() for x in pred]\n","print(f\"TEST F-1: {f1_score(true, pred, average='weighted')}\")\n","print(f\"TEST ACC: {accuracy_score(true, pred)}\")"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"no_downloaded⁄fasttext.ipynb","provenance":[]},"interpreter":{"hash":"8abf625e542ff33194dd4502f291ce11b7bf8daed732e6d5f31b5a030b27ce17"},"kernelspec":{"display_name":"Python 3.9.5 64-bit ('base': conda)","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.5"}},"nbformat":4,"nbformat_minor":5}
