{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "fasttext.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "interpreter": {
      "hash": "8abf625e542ff33194dd4502f291ce11b7bf8daed732e6d5f31b5a030b27ce17"
    },
    "kernelspec": {
      "display_name": "Python 3.9.5 64-bit ('base': conda)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "1yv5wo80zFFD"
      },
      "source": [
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "import regex\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "random.seed(RANDOM_SEED)\n",
        "np.random.seed(RANDOM_SEED)\n",
        "\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "id": "1yv5wo80zFFD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_4lpUGTQzFFI"
      },
      "source": [
        "def split_train_val_test(df, props=[.8, .1, .1]):\n",
        "    assert round(sum(props), 2) == 1 and len(props) >= 2\n",
        "    train_df, test_df, val_df = None, None, None\n",
        "\n",
        "    train_size = int(props[0] * len(df))\n",
        "    val_size =  train_size + int(props[1] * len(df))\n",
        "    test_size =val_size + int(props[2] * len(df)) \n",
        "    train_df = df.iloc[0:train_size]\n",
        "    val_df = df.iloc[train_size:val_size]\n",
        "    test_df = df.iloc[val_size:test_size]\n",
        "    \n",
        "    return train_df, val_df, test_df"
      ],
      "id": "_4lpUGTQzFFI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u04VW4ugzFFI"
      },
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "def download_embeddings(fasttetxt):\n",
        "    # https://fasttext.cc/docs/en/english-vectors.html\n",
        "    if fasttetxt:\n",
        "      wv = api.load(\"fasttext-wiki-news-subwords-300\")\n",
        "    else:\n",
        "      \n",
        "      wv = api.load(\"word2vec-google-news-300\")\n",
        "      print(\"\\nLoading complete!\\n\" +\n",
        "            \"Vocabulary size: {}\".format(len(wv.vocab)))\n",
        "    return wv\n"
      ],
      "id": "u04VW4ugzFFI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23WpDX0rzFFJ",
        "outputId": "134fd84b-2cdf-4e9f-939e-c8b5946583a5"
      },
      "source": [
        "# Opening and preprocessing input file\n",
        "import gensim.models\n",
        "import pandas as pd\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from tqdm import tqdm\n",
        "from preprocess import clean_text\n",
        "\n",
        "data = pd.read_csv('train.csv', quotechar='\"')\n",
        "data.sample(frac=1)\n",
        "\n",
        "\n",
        "# to convert authors into numbers\n",
        "author_to_number = {\n",
        "    'EAP': 0,\n",
        "    'HPL': 1,\n",
        "    'MWS': 2\n",
        "    \n",
        "}\n",
        "\n",
        "# lowercase, removing punctuation and tookenize sentences. Converting labels to int\n",
        "training_text = \"\"\n",
        "for i in range(len(data)):\n",
        "\n",
        "    data['text'][i] = nltk.word_tokenize(regex.sub(r'[^\\w\\s]', '',data['text'][i].lower()))\n",
        "    data['author'][i] = author_to_number[data['author'][i]]\n",
        "\n",
        "print(data[0:10])\n",
        "print(len(data))\n",
        "\n",
        "from dataset import *\n",
        "\n",
        "# Splitting dataset and generating vocab\n",
        "train_df, val_df, test_df = split_train_val_test(data)\n",
        "train_vocab, reversed_vocab = generate_vocab_map(train_df)\n",
        "\n"
      ],
      "id": "23WpDX0rzFFJ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "        id                                               text author\n",
            "0  id26305  [this, process, however, afforded, me, no, mea...      0\n",
            "1  id17569  [it, never, once, occurred, to, me, that, the,...      1\n",
            "2  id11008  [in, his, left, hand, was, a, gold, snuff, box...      0\n",
            "3  id27763  [how, lovely, is, spring, as, we, looked, from...      2\n",
            "4  id12958  [finding, nothing, else, not, even, gold, the,...      1\n",
            "5  id22965  [a, youth, passed, in, solitude, my, best, yea...      2\n",
            "6  id09674  [the, astronomer, perhaps, at, this, point, to...      0\n",
            "7  id13515  [the, surcingle, hung, in, ribands, from, my, ...      0\n",
            "8  id19322  [i, knew, that, you, could, not, say, to, your...      0\n",
            "9  id00912  [i, confess, that, neither, the, structure, of...      2\n",
            "19579\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NTDiU36-zFFK"
      },
      "source": [
        "DOWNLOAD = True\n",
        "# Use fastext or word2vec\n",
        "FASTTEXT = True\n",
        "WINDOW_SIZE = 5\n",
        "\n",
        "EMBEDDING_DIM = 300\n",
        "HIDDEN_DIM = 128\n",
        "NUM_LAYERS = 1\n",
        "BIDIRECTIONAL = True\n"
      ],
      "id": "NTDiU36-zFFK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kaqaR8CNzFFK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16e88b70-2e13-4a2f-c121-a1aca802e146"
      },
      "source": [
        "# Downloading or generating word2vec embeddings\n",
        "\n",
        "if DOWNLOAD:\n",
        "    model = download_embeddings(FASTTEXT)\n",
        "else:\n",
        "    if FASTTEXT:\n",
        "        model = gensim.models.FastText(sentences=train_df['text'], size=EMBEDDING_DIM, window=WINDOW_SIZE)\n",
        "    else:\n",
        "        model = gensim.models.Word2Vec(sentences=train_df['text'], size=EMBEDDING_DIM, window=WINDOW_SIZE)\n",
        "                        "
      ],
      "id": "kaqaR8CNzFFK",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 958.5/958.4MB downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fVz5UQwTzFFL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d0f1b27-73c6-4a64-8187-7febac3b0cd2"
      },
      "source": [
        "from dataset import HeadlineDataset\n",
        "from torch.utils.data import RandomSampler\n",
        "\n",
        "train_dataset = HeadlineDataset(train_vocab, train_df,model.wv, FASTTEXT)\n",
        "val_dataset = HeadlineDataset(train_vocab, val_df,model.wv, FASTTEXT)\n",
        "test_dataset = HeadlineDataset(train_vocab, test_df,model.wv, FASTTEXT)\n",
        "\n",
        "# Now that we're wrapping our dataframes in PyTorch datsets, we can make use of PyTorch Random Samplers.\n",
        "train_sampler = RandomSampler(train_dataset)\n",
        "val_sampler = RandomSampler(val_dataset)\n",
        "test_sampler = RandomSampler(test_dataset)"
      ],
      "id": "fVz5UQwTzFFL",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
            "  after removing the cwd from sys.path.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
            "  \"\"\"\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
            "  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gnfaq2kpzFFM",
        "outputId": "abf89d96-b2fd-441d-e72a-b4e3c78c5b37"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from dataset import collate_fn\n",
        "BATCH_SIZE = 16\n",
        "train_iterator = DataLoader(train_dataset, batch_size=BATCH_SIZE, sampler=train_sampler, collate_fn=collate_fn)\n",
        "val_iterator = DataLoader(val_dataset, batch_size=BATCH_SIZE, sampler=val_sampler, collate_fn=collate_fn)\n",
        "test_iterator = DataLoader(test_dataset, batch_size=BATCH_SIZE, sampler=test_sampler, collate_fn=collate_fn)\n",
        "\n",
        "for x, y in test_iterator:\n",
        "    print(x,y)\n",
        "    break"
      ],
      "id": "gnfaq2kpzFFM",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[-7.9206e-03, -9.5293e-02,  3.1266e-02,  ..., -8.4443e-03,\n",
            "          -6.6989e-02,  2.5416e-02],\n",
            "         [-1.1860e-02, -4.0471e-02,  3.1929e-03,  ..., -8.9638e-03,\n",
            "          -4.1890e-02, -9.5504e-02],\n",
            "         [ 6.8832e-03, -6.3845e-02,  8.0359e-02,  ..., -1.8366e-02,\n",
            "          -4.0415e-02,  1.0409e-02],\n",
            "         ...,\n",
            "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "           0.0000e+00,  0.0000e+00],\n",
            "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "           0.0000e+00,  0.0000e+00],\n",
            "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "           0.0000e+00,  0.0000e+00]],\n",
            "\n",
            "        [[-6.0418e-02,  6.9955e-02,  5.3173e-02,  ..., -4.7788e-03,\n",
            "           3.3832e-02, -2.0630e-01],\n",
            "         [ 1.8724e-03, -2.7258e-02, -2.1065e-02,  ...,  4.3932e-02,\n",
            "          -2.8055e-03, -4.8145e-03],\n",
            "         [ 1.8347e-02,  2.9830e-02,  8.0032e-03,  ..., -4.2948e-02,\n",
            "          -2.0935e-02, -2.5170e-02],\n",
            "         ...,\n",
            "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "           0.0000e+00,  0.0000e+00],\n",
            "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "           0.0000e+00,  0.0000e+00],\n",
            "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "           0.0000e+00,  0.0000e+00]],\n",
            "\n",
            "        [[-3.9255e-02,  6.2436e-02,  1.6717e-02,  ..., -1.2150e-02,\n",
            "           1.9683e-02,  1.7469e-02],\n",
            "         [-2.8408e-02, -1.6403e-02,  9.9537e-03,  ...,  2.3913e-02,\n",
            "          -3.5907e-03, -5.4667e-02],\n",
            "         [ 2.1714e-02,  6.0647e-02,  6.6695e-03,  ..., -4.5046e-03,\n",
            "          -5.0133e-02,  2.9674e-02],\n",
            "         ...,\n",
            "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "           0.0000e+00,  0.0000e+00],\n",
            "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "           0.0000e+00,  0.0000e+00],\n",
            "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "           0.0000e+00,  0.0000e+00]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-7.9206e-03, -9.5293e-02,  3.1266e-02,  ..., -8.4443e-03,\n",
            "          -6.6989e-02,  2.5416e-02],\n",
            "         [-9.2966e-02,  1.6000e-02, -1.1183e-02,  ..., -7.6578e-02,\n",
            "           3.9478e-02,  6.1293e-02],\n",
            "         [ 6.8832e-03, -6.3845e-02,  8.0359e-02,  ..., -1.8366e-02,\n",
            "          -4.0415e-02,  1.0409e-02],\n",
            "         ...,\n",
            "         [ 1.7057e-02, -1.3480e-02,  1.5958e-02,  ..., -4.4837e-02,\n",
            "          -5.5900e-02,  2.1912e-03],\n",
            "         [ 3.1941e-03, -1.2890e-01,  1.1445e-01,  ...,  3.8478e-02,\n",
            "          -2.7846e-02,  2.6440e-02],\n",
            "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "           0.0000e+00,  0.0000e+00]],\n",
            "\n",
            "        [[ 6.2401e-02,  6.9159e-02,  5.3561e-02,  ..., -1.2549e-01,\n",
            "          -6.8716e-02, -2.8681e-02],\n",
            "         [ 8.6538e-03, -1.0483e-01,  5.4545e-02,  ..., -5.7892e-02,\n",
            "          -2.8059e-02,  8.7626e-02],\n",
            "         [-3.7381e-02,  5.8914e-02, -1.5221e-02,  ..., -3.9729e-02,\n",
            "          -9.5309e-03, -3.2581e-02],\n",
            "         ...,\n",
            "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "           0.0000e+00,  0.0000e+00],\n",
            "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "           0.0000e+00,  0.0000e+00],\n",
            "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "           0.0000e+00,  0.0000e+00]],\n",
            "\n",
            "        [[ 2.1340e-02, -2.9784e-02,  4.0022e-02,  ..., -2.7690e-02,\n",
            "           2.0860e-02, -3.1198e-02],\n",
            "         [-1.4953e-02, -3.7543e-03, -1.0864e-02,  ...,  1.7516e-04,\n",
            "           2.1659e-02, -4.7073e-02],\n",
            "         [-2.1026e-02,  1.0842e-02,  2.5643e-02,  ..., -2.2459e-02,\n",
            "           3.1234e-02, -5.8465e-02],\n",
            "         ...,\n",
            "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "           0.0000e+00,  0.0000e+00],\n",
            "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "           0.0000e+00,  0.0000e+00],\n",
            "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "           0.0000e+00,  0.0000e+00]]]) tensor([0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 2, 2, 2, 2, 0, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FvFz28NgzFFM"
      },
      "source": [
        "### Modeling"
      ],
      "id": "FvFz28NgzFFM"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E54k4vH-zFFO",
        "outputId": "2a93c456-aa84-4768-d020-e9b8fd79c506"
      },
      "source": [
        "from models import ClassificationModel\n",
        "\n",
        "model = ClassificationModel(len(train_vocab),embedding_dim=EMBEDDING_DIM,hidden_dim = HIDDEN_DIM,num_layers = NUM_LAYERS,bidirectional = BIDIRECTIONAL)\n",
        "\n",
        "model.to(device)"
      ],
      "id": "E54k4vH-zFFO",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ClassificationModel(\n",
              "  (LSTM): LSTM(300, 128, batch_first=True, bidirectional=True)\n",
              "  (linear): Linear(in_features=256, out_features=3, bias=True)\n",
              "  (softmax): Softmax(dim=1)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PeoanbgVzFFO"
      },
      "source": [
        "In the following cell, **instantiate the model with some hyperparameters, and select an appropriate loss function and optimizer.** \n",
        "\n",
        "Hint: we already use sigmoid in our model. What loss functions are availible for binary classification? Feel free to look at PyTorch docs for help!"
      ],
      "id": "PeoanbgVzFFO"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4rcqsWzHzFFP"
      },
      "source": [
        "from torch.optim import AdamW\n",
        "\n",
        "criterion, optimizer = torch.nn.CrossEntropyLoss(), torch.optim.Adam(model.parameters(), lr=0.001)"
      ],
      "id": "4rcqsWzHzFFP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNFrnFAVzFFP"
      },
      "source": [
        "### Part 3: Training and Evaluation [10 Points]\n",
        "The final part of this HW involves training the model, and evaluating it at each epoch. **Fill out the train and test loops below.**"
      ],
      "id": "kNFrnFAVzFFP"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4WOY7B-azFFP"
      },
      "source": [
        "# returns the total loss calculated from criterion\n",
        "def train_loop(model, criterion, iterator):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    \n",
        "    for x, y in tqdm(iterator):\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        prediction = model(x)\n",
        "        prediction = torch.squeeze(prediction)\n",
        "        # y = y.round()\n",
        "        y = y.long()\n",
        "        \n",
        "\n",
        " \n",
        "        loss = criterion(prediction,y)\n",
        "        total_loss += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    return total_loss\n",
        "\n",
        "# returns:\n",
        "# - true: a Python boolean array of all the ground truth values \n",
        "#         taken from the dataset iterator\n",
        "# - pred: a Python boolean array of all model predictions. \n",
        "def val_loop(model, criterion, iterator):\n",
        "    true, pred = [], []\n",
        "    for x, y in tqdm(iterator):\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "    \n",
        "        preds = model(x)\n",
        "        preds.to(device)\n",
        "        preds = torch.squeeze(preds)\n",
        "        for i_batch in range(len(y)):\n",
        "            true.append(y[i_batch])\n",
        "            pred.append(torch.argmax(preds[i_batch]))\n",
        "            \n",
        "    return true, pred\n"
      ],
      "id": "4WOY7B-azFFP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RwyLMPAwzFFQ",
        "outputId": "291baaec-1b38-40f4-f9fa-01050ae967ae"
      },
      "source": [
        "# To test your eval implementation, let's see how well the untrained model does on our dev dataset.\n",
        "# It should do pretty poorly.\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "\n",
        "from eval_utils import binary_macro_f1, accuracy\n",
        "true, pred = val_loop(model, criterion, val_iterator)\n",
        "true = [x.item() for x in true]\n",
        "pred = [x.item() for x in pred]\n",
        "print(f1_score(true, pred, average='weighted'))\n",
        "print(accuracy_score(true, pred))\n"
      ],
      "id": "RwyLMPAwzFFQ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 123/123 [00:04<00:00, 26.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.23522685725016632\n",
            "0.3955033214103219\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "klS-hqCezFFQ"
      },
      "source": [
        "### Actually training the model"
      ],
      "id": "klS-hqCezFFQ"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PS8EQy8SzFFQ",
        "outputId": "0ed5e517-59c6-4653-f7db-f3dda2c24282"
      },
      "source": [
        "TOTAL_EPOCHS = 10\n",
        "for epoch in range(TOTAL_EPOCHS):\n",
        "    train_loss = train_loop(model, criterion, train_iterator)\n",
        "    true, pred = val_loop(model, criterion, val_iterator)\n",
        "    true = [x.item() for x in true]\n",
        "    pred = [x.item() for x in pred]\n",
        "    print(f\"EPOCH: {epoch}\")\n",
        "    print(f\"TRAIN LOSS: {train_loss}\")\n",
        "    print(f\"VAL F-1: {f1_score(true, pred, average='weighted')}\")\n",
        "    print(f\"VAL ACC: {accuracy_score(true, pred)}\")\n"
      ],
      "id": "PS8EQy8SzFFQ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 979/979 [02:03<00:00,  7.93it/s]\n",
            "100%|██████████| 123/123 [00:04<00:00, 27.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 0\n",
            "TRAIN LOSS: 895.5107879340649\n",
            "VAL F-1: 0.6857177569132743\n",
            "VAL ACC: 0.6852324987225344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 979/979 [02:03<00:00,  7.96it/s]\n",
            "100%|██████████| 123/123 [00:04<00:00, 27.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 1\n",
            "TRAIN LOSS: 712.9672981202602\n",
            "VAL F-1: 0.707567152104528\n",
            "VAL ACC: 0.7072049054675523\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 979/979 [02:02<00:00,  8.02it/s]\n",
            "100%|██████████| 123/123 [00:04<00:00, 27.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 2\n",
            "TRAIN LOSS: 649.9884182810783\n",
            "VAL F-1: 0.710661844661212\n",
            "VAL ACC: 0.7133367399080225\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 979/979 [02:02<00:00,  7.96it/s]\n",
            "100%|██████████| 123/123 [00:04<00:00, 27.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 3\n",
            "TRAIN LOSS: 601.2533829659224\n",
            "VAL F-1: 0.7344570190038997\n",
            "VAL ACC: 0.7353091466530404\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 979/979 [02:03<00:00,  7.93it/s]\n",
            "100%|██████████| 123/123 [00:04<00:00, 27.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 4\n",
            "TRAIN LOSS: 564.2798029780388\n",
            "VAL F-1: 0.7440304921032496\n",
            "VAL ACC: 0.7445068983137455\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 979/979 [02:02<00:00,  8.02it/s]\n",
            "100%|██████████| 123/123 [00:04<00:00, 27.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 5\n",
            "TRAIN LOSS: 526.9463529139757\n",
            "VAL F-1: 0.7555531519004008\n",
            "VAL ACC: 0.7557485947879408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 979/979 [02:02<00:00,  8.01it/s]\n",
            "100%|██████████| 123/123 [00:04<00:00, 27.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 6\n",
            "TRAIN LOSS: 494.69645085930824\n",
            "VAL F-1: 0.7508706008814656\n",
            "VAL ACC: 0.7521716913643332\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 979/979 [02:01<00:00,  8.05it/s]\n",
            "100%|██████████| 123/123 [00:04<00:00, 27.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 7\n",
            "TRAIN LOSS: 456.7603053227067\n",
            "VAL F-1: 0.7509536195783306\n",
            "VAL ACC: 0.7511497189575882\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 979/979 [02:01<00:00,  8.04it/s]\n",
            "100%|██████████| 123/123 [00:04<00:00, 27.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 8\n",
            "TRAIN LOSS: 419.0230185240507\n",
            "VAL F-1: 0.7632571538785019\n",
            "VAL ACC: 0.7639243740419008\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 979/979 [02:02<00:00,  8.00it/s]\n",
            "100%|██████████| 123/123 [00:04<00:00, 27.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 9\n",
            "TRAIN LOSS: 383.1611753106117\n",
            "VAL F-1: 0.7734002201000063\n",
            "VAL ACC: 0.7736331119059785\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECkuloBSzFFR"
      },
      "source": [
        "We can also look at the models performance on the held-out test set, using the same val_loop we wrote earlier."
      ],
      "id": "ECkuloBSzFFR"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E9S33DF4zFFR",
        "outputId": "a58c056a-8409-454c-eadc-b035e282f621"
      },
      "source": [
        "true, pred = val_loop(model, criterion, test_iterator)\n",
        "true = [x.item() for x in true]\n",
        "pred = [x.item() for x in pred]\n",
        "print(f\"TEST F-1: {f1_score(true, pred, average='weighted')}\")\n",
        "print(f\"TEST ACC: {accuracy_score(true, pred)}\")"
      ],
      "id": "E9S33DF4zFFR",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 123/123 [00:04<00:00, 27.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TEST F-1: 0.7619887091381111\n",
            "TEST ACC: 0.7623914154317833\n"
          ]
        }
      ]
    }
  ]
}